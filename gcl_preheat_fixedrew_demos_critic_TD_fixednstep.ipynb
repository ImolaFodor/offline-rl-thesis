{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import myModelinTF\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from torch.autograd import Variable \n",
        "\n",
        "from statistics import mean \n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train firsts set size: 495\n"
          ]
        }
      ],
      "source": [
        "demo = np.load('dataset_train_firsts.npy', allow_pickle=True)\n",
        "print(\"Train firsts set size: \" + str(len(demo)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def permute_them(x):\n",
        "#   return torch.permute(x, (1, 0, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fill_four_with_last(arr):\n",
        "    prev = np.arange(len(arr))\n",
        "    prev[arr == 4] = 0\n",
        "    prev = np.maximum.accumulate(prev)\n",
        "\n",
        "    # it can be that the first element is 4, so remove those\n",
        "    temp = arr[prev]\n",
        "    new_deleted = np.delete(temp, np.where(temp == 4))\n",
        "    count = len(temp) - len(new_deleted)\n",
        "    new = np.insert(new_deleted, 0, np.repeat(new_deleted[0], count), axis=0)\n",
        "    return new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PG(nn.Module):\n",
        "    def __init__(self, n_actions):\n",
        "        super().__init__()\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        # Policy takes input initial conditions and outputs planning route, open-loop control\n",
        "        self.actor_model = nn.Sequential(\n",
        "            nn.LSTM(input_size = 1, hidden_size = self.n_actions, num_layers = 2, dropout = 0.1,batch_first = True),\n",
        "        )\n",
        "\n",
        "        self.logprobs = []\n",
        "        self.state_values = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, states):\n",
        "        logits, (h_T, c_T)  = self.actor_model(states)\n",
        "        return logits[-1]\n",
        "    \n",
        "    # Run agent in environment to create sample trajectories by generator\n",
        "    # The environment model is a seq2seq model\n",
        "    def rollout_policy(self, sysmodel, policy, init, traj_idx, t_step = 20, t_max=200):\n",
        "\n",
        "        states, actions, actions_probs_policy, rewards = [], [], [], []\n",
        "        init_idx = init[traj_idx]\n",
        "        states_init_idx = init_idx[0]\n",
        "        init_COT_idx = states_init_idx[1,0:t_step]\n",
        "\n",
        "        states = torch.FloatTensor(init_COT_idx.T)\n",
        "\n",
        "        states_history = states.unsqueeze(0)\n",
        "        #states_history = states[-1,:].unsqueeze(0)\n",
        "        list_actions_taken = []\n",
        "        actions_probs_policy_taken = []\n",
        "        action = 0\n",
        "        for t in range(0, t_max, t_step):\n",
        "            logits = policy(states_history) # forward pass\n",
        "            \n",
        "            probs = nn.functional.softmax(logits, -1) # get estimated actions for states\n",
        "\n",
        "            actions_probs_policy = np.squeeze(probs).clone().detach().numpy()\n",
        "            actions_probs_policy_rep = np.tile(actions_probs_policy,(t_step,1))\n",
        "            actions_probs_policy_taken.append(actions_probs_policy_rep)\n",
        "\n",
        "            action_prev = action\n",
        "            action = np.random.choice(3,  p = actions_probs_policy)\n",
        "            if action == 2:\n",
        "                action = action_prev\n",
        "\n",
        "            action_rep = np.repeat(action, t_step)\n",
        "            list_actions_taken.append(action_rep)\n",
        "            \n",
        "            #list_actions_filled = np.append(list_actions_filled, a)\n",
        "            \n",
        "            # Apply dict to go from 1,2,3,4 as action to [0,2400,0] [0,2400,1000]  [2300,0,0]  [2300,0,1000] \n",
        "            my_dict = {0:[0,2400,1000], 1:[2300,0,1000], 2: [2300,0,0]} \n",
        "            list_actions_taken_np = np.array(list_actions_taken) \n",
        "            list_actions_taken_np = np.append([2, 2, 2, 2, 2], list_actions_taken_np)\n",
        "            actions_array_mapped = map(my_dict.get, list_actions_taken_np.flatten())\n",
        "            actions_array_mapped = np.array(list(actions_array_mapped))\n",
        "            \n",
        "            actions_array_mapped = (actions_array_mapped - norm_params_actions[:,0]) / norm_params_actions[:,1]\n",
        "\n",
        "            actions_sysmodel = np.concatenate([np.arange(actions_array_mapped.shape[0])[:,None]+1,actions_array_mapped], axis=1) #maybe needed [:,None]\n",
        "            actions_sysmodel = tf.expand_dims(actions_sysmodel, axis = 0, name=None)\n",
        "            \n",
        "            states = sysmodel(actions_sysmodel)\n",
        "\n",
        "            # Revert normalized values for states\n",
        "\n",
        "            states = states*norm_params[:,1]+norm_params[:,0]\n",
        "            states_COT_np = np.array(states[:,:,0])\n",
        "            states = torch.FloatTensor(states_COT_np[:,:-1])\n",
        "            # More actions fed to the sysmodel, higher the accuracy, but the resulting states will not be the ones effectively experienced, so history is needed\n",
        "            # Rewards can be calculated on the set of states experienced with all actions at once, but then the scope is just to evaluate the set of actions in the whole episode, not step by step\n",
        "            states_history = torch.cat((states, states[-t_step:]), 1)\n",
        "\n",
        "            # if done:\n",
        "            #     break\n",
        "        \n",
        "        # print(actions_taken)\n",
        "        # actions_taken = np.array(actions_taken)\n",
        "        actions_probs_policy_taken = np.array(actions_probs_policy_taken).reshape(t_max, 3)\n",
        "        # states_all = sysmodel(actions_taken_reshape)\n",
        "        # states_all = states_new*norm_params[:,0]+norm_params[:,1]\n",
        "        rewards = get_rewards(states, np.array(list_actions_taken), t_max)\n",
        "        return states_history, states, list_actions_taken_np, actions_probs_policy_taken, rewards\n",
        "    \n",
        "    def train(self, sysmodel, policy, critic, init, traj_idx, t_step = 20, t_max=200, gamma = 0.9):\n",
        "        #policy.train()\n",
        "\n",
        "        states, actions, actions_probs_policy, rewards, list_actions_taken, actions_probs_policy_taken = [], [], [], [], [], []\n",
        "        init_idx = init[traj_idx]\n",
        "        states_init_idx = init_idx[0]\n",
        "        actions_init_idx = init_idx[1]\n",
        "        init_COT_idx = states_init_idx[1,0:t_step]\n",
        "\n",
        "        states = torch.FloatTensor(init_COT_idx.T)\n",
        "\n",
        "        states_history = states.unsqueeze(0)\n",
        "\n",
        "        action = 0\n",
        "\n",
        "        step = 1\n",
        "        for t in range(0, t_max, t_step):\n",
        "            \n",
        "            # Forward pass policy\n",
        "            logits = policy(states_history) \n",
        "            \n",
        "            # Get estimated actions for states in scale [0,1]\n",
        "            probs = nn.functional.softmax(logits, -1) \n",
        "\n",
        "            # Keep action probs for t_step - Repeated action concept\n",
        "            actions_probs_policy = np.squeeze(probs).clone().detach().numpy()\n",
        "            actions_probs_policy_rep = np.tile(actions_probs_policy,(t_step,1))\n",
        "            actions_probs_policy_taken.append(actions_probs_policy_rep)\n",
        "\n",
        "            # Action 2 has the meaning \"Keep previous action\"\n",
        "            # The other two actions : 0:[0,2400,1000], 1:[2300,0,1000]\n",
        "            action_prev = action\n",
        "            action = np.random.choice(3,  p = actions_probs_policy)\n",
        "            if action == 2:\n",
        "                action = action_prev\n",
        "\n",
        "            # Keep action for t_step too - Repeated action concept\n",
        "            action_rep = np.repeat(action, t_step)\n",
        "            list_actions_taken.append(action_rep)\n",
        "            \n",
        "            # Apply dict to go from 1,2,3,4 as action to [0,2400,0] [0,2400,1000]  [2300,0,0]  [2300,0,1000] \n",
        "            my_dict = {0:[0,2400,1000], 1:[2300,0,1000], 2: [2300,0,0]} \n",
        "            list_actions_taken_np = np.array(list_actions_taken) \n",
        "            list_actions_taken_np = np.append([2, 2, 2, 2, 2], list_actions_taken_np)\n",
        "            list_actions_taken_np = list_actions_taken_np.flatten()\n",
        "            actions_array_mapped = map(my_dict.get, list_actions_taken_np)\n",
        "            actions_array_mapped = np.array(list(actions_array_mapped))\n",
        "            \n",
        "            # Normalize actions wrt to also states, mean, stdev calculated on offline CFD data\n",
        "            actions_array_mapped = (actions_array_mapped - norm_params_actions[:,0]) / norm_params_actions[:,1]\n",
        "\n",
        "            # The input to the sysmodel is also an absolute time index\n",
        "            actions_sysmodel = np.concatenate([np.arange(actions_array_mapped.shape[0])[:,None]+1,actions_array_mapped], axis=1) \n",
        "            actions_sysmodel = tf.expand_dims(actions_sysmodel, axis = 0, name=None)\n",
        "            \n",
        "            # Take a step in the environment **with all previous actions taken**\n",
        "            states = sysmodel(actions_sysmodel)\n",
        "\n",
        "            # Revert normalization step for states\n",
        "            states = states*norm_params[:,1]+norm_params[:,0]\n",
        "\n",
        "            # From output consider only COT value\n",
        "            states_COT_np = np.array(states[:,:,0])\n",
        "            states = torch.FloatTensor(states_COT_np[:,:-1])\n",
        "            \n",
        "            # States is the output from sysmodel, \n",
        "            # states_history is incrementally saving states at every t_step \n",
        "            states_history = torch.cat((states_history, states[-t_step:]), 1)\n",
        "\n",
        "            # Flatten \n",
        "            actions_probs_policy_taken_np = np.array(actions_probs_policy_taken).reshape(step*t_step, 3)\n",
        "\n",
        "            # Calculate Loss Actor and Critic\n",
        "\n",
        "            # Skip first iter, since there is no \"previous value\"\n",
        "            if t > 4*t_step:\n",
        "\n",
        "                # When the scope is to get an action sequence only, use states for get_rewards()\n",
        "                rewards = get_rewards(states[:, -t_step*2:-t_step], np.array(list_actions_taken), t_max)\n",
        "\n",
        "                # Actor\n",
        "                states_np = states.numpy()\n",
        "\n",
        "                input1 = states_np[:,-1]  \n",
        "                for i in range(4):\n",
        "                    input1 = np.concatenate((input1, [np.max(states_np[:,-5*(i+1):-5*(i)-1])]))\n",
        "                v_prime = critic(input1)\n",
        "\n",
        "                input2 = states_np[:,-t_step]\n",
        "                for i in range(4):\n",
        "                    k = i+6\n",
        "                    input2 = np.concatenate((input2, [np.max(states_np[:,-5*k:-5*(k-1)])]))\n",
        "                v = critic(input2)\n",
        "                \n",
        "                gamma_r = []\n",
        "                for i in range(t_step):\n",
        "                    expt = t_step + i -t_step\n",
        "                    gamma_r.append(gamma**expt)\n",
        "\n",
        "                td_error_v = torch.sum(torch.from_numpy(np.array(gamma_r)) * rewards) + v_prime - (gamma**t_step)*v\n",
        "\n",
        "                actions_tensor = torch.from_numpy(list_actions_taken_np)\n",
        "                probs_tensor = torch.from_numpy(np.array(actions_probs_policy_taken_np))\n",
        "\n",
        "                log_probs_for_actions = torch.log(torch.sum(\n",
        "                    probs_tensor * to_one_hot(actions_tensor[5:], 3), dim =1))\n",
        "                \n",
        "                # UPDATING THE POLICY NETWORK\n",
        "                loss_policy = -(torch.mean(log_probs_for_actions)*td_error_v).sum()\n",
        "\n",
        "                optimizer_policy.zero_grad()\n",
        "                #loss_policy.requires_grad = True\n",
        "                loss_policy.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(policy.parameters(), 0.01)\n",
        "                optimizer_policy.step()\n",
        "\n",
        "                # Critic\n",
        "\n",
        "                # target = torch.sum(torch.from_numpy(np.array(gamma_r))*rewards) + v_prime\n",
        "                # predicted = v\n",
        "                # # UPDATING THE VALUE NETWORK\n",
        "                # loss_critic = criterionMSE(predicted.to(torch.float32), target.to(torch.float32))/1e3\n",
        "                \n",
        "                # optimizer_critic.zero_grad()\n",
        "                # #loss_critic.requires_grad = True\n",
        "                # loss_critic.backward()\n",
        "                # torch.nn.utils.clip_grad_norm_(critic.parameters(), 0.01)\n",
        "                # optimizer_critic.step()\n",
        "\n",
        "                #returns = sum(rewards)\n",
        "                sum_of_loss = loss_policy.item()\n",
        "                print(f\"\\nLoss Actor: {sum_of_loss}\\n\")\n",
        "                #print(f\"\\nLoss Critic: {loss_critic.item()}\\n\")\n",
        "                \n",
        "\n",
        "            step += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.critic_model = nn.Sequential(\n",
        "            nn.Linear(5, 32),\n",
        "            nn.LeakyReLU(inplace=False),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = torch.FloatTensor(state) #.unsqueeze(0)\n",
        "        state_value = self.critic_model(state)\n",
        "        return state_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ind(array, t_max):\n",
        "    idx_return = (t_max + 1,0)\n",
        "    for idx, val in np.ndenumerate(array):\n",
        "        if val <= 201 and val >= 199:\n",
        "            idx_return = idx\n",
        "            break\n",
        "    return int(idx_return[0])\n",
        "    # If no item was found return None, other return types might be a problem due to\n",
        "    # numbas type inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_DYTtGk7Fa1h"
      },
      "outputs": [],
      "source": [
        "def get_rewards(states, actions, t_max):\n",
        "    \n",
        "    states = states.numpy()\n",
        "    states = states.squeeze()\n",
        "    rewards = np.zeros((np.shape(states)[0],1))\n",
        "\n",
        "    actions = actions.flatten()\n",
        "\n",
        "    # print(actions)\n",
        "    max_hotspot =  np.max(states)\n",
        "    if max_hotspot < 280:\n",
        "        rewards[-1] = 1\n",
        "\n",
        "    target_first_reached_at = ind(states,t_max)\n",
        "\n",
        "    print(f\"Reached Target 200C at step {target_first_reached_at} with t_max = {t_max}\")\n",
        "\n",
        "    if target_first_reached_at < t_max and target_first_reached_at > (t_max - 180):\n",
        "        rewards[-1] = rewards[-1] + 15\n",
        "\n",
        "        \n",
        "    for idx in range(np.shape(states)[0]):\n",
        "        #idx = idx -1\n",
        "        # if temps.any() > 230:\n",
        "        #     rewards[idx] = rewards[idx] -30\n",
        "        \n",
        "        # subset = actions[idx-20:]\n",
        "        # if np.sum(np.diff(subset)) == 0:\n",
        "        #     rewards[idx] = rewards[idx] + 20\n",
        "\n",
        "        # subset = states[idx-10:]\n",
        "        # if np.absolute(np.sum(np.diff(subset))) < 10:\n",
        "        #     #print('Difference between states not big')\n",
        "        #     rewards[idx] = rewards[idx] + 70\n",
        "\n",
        "        # Reduce power consumption\n",
        "        # if actions[idx] == 1 | actions[idx] == 3:\n",
        "        #     rewards[idx] = rewards[idx] -10\n",
        "\n",
        "        # Action 4 is encouraged , has the meaning to keep the previous action\n",
        "        # if actions[idx] == 2:\n",
        "        #     #print('Reward for keeping previous action')\n",
        "        #     rewards[idx] = rewards[idx] + 1\n",
        "\n",
        "        # Negative temp goes with high negative reward\n",
        "        if states[idx] < 0:\n",
        "            #print('Penalizing for negative values')\n",
        "            rewards[idx] = rewards[idx] -2\n",
        "\n",
        "        # Penalize if previous state higher than current (variation)\n",
        "        temps_subset = np.array([states[idx], states[idx-1]])\n",
        "        if np.diff(temps_subset) <= 0:\n",
        "            #print('Penalizing for decreasing or steady state value')\n",
        "            rewards[idx] = rewards[idx] - 5\n",
        "\n",
        "        if 0 > np.diff(temps_subset) <= 1 :\n",
        "            #print('Penalizing for decreasing or steady state value')\n",
        "            rewards[idx] = rewards[idx] + 2\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def to_one_hot(y_tensor, ndims):\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_demos(demos, traj_idx):\n",
        "    demo = demos[traj_idx]\n",
        "    states_demo = demo[0]\n",
        "    \n",
        "    COT_demo = states_demo[1]\n",
        "    states = torch.FloatTensor(COT_demo.T)\n",
        "    states = states.unsqueeze(0)\n",
        "\n",
        "    list_actions_taken = np.array(demo[1])\n",
        "    actions_probs_policy_taken = demo[2]\n",
        "\n",
        "    rewards = get_rewards(states, list_actions_taken, 480)\n",
        "    \n",
        "    return states, list_actions_taken, actions_probs_policy_taken, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IM1Ki1hL2U42",
        "outputId": "975040a9-996f-4879-cf39-9c3f89ebbc25"
      },
      "outputs": [],
      "source": [
        "# DEVICE\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "# ENV SETUP\n",
        "sysmodel = myModelinTF.load_model()\n",
        "n_actions = 3 # excluded 1:[0,2400,1000], 3:[2300,0,1000], then returned now excluded {0:[0,2400,0], 1:[2300,0,0]}\n",
        "\n",
        "# INITILIZING POLICY AND REWARD FUNCTION\n",
        "policy = PG(n_actions)\n",
        "policy.to(DEVICE)\n",
        "\n",
        "critic = Critic()\n",
        "critic.to(DEVICE)\n",
        "\n",
        "norm_params = pd.read_csv('D:\\\\units\\\\thesis\\\\thesis\\\\train_reward_inverse_rl_w_sys_model\\\\norm_params.csv', header = None).to_numpy().T\n",
        "norm_params_actions = pd.read_csv('D:\\\\units\\\\thesis\\\\thesis\\\\train_reward_inverse_rl_w_sys_model\\\\norm_params_actions.csv', header = None).to_numpy().T\n",
        "\n",
        "\n",
        "# LOADING EXPERT/DEMO SAMPLES\n",
        "DATASETNPY = \"dataset_train_firsts.npy\"\n",
        "\n",
        "DATASET_DEMO = \"dataset_train.npy\"\n",
        "\n",
        "# Mean stdev from CFD sims\n",
        "norm_params = pd.read_csv('D:\\\\units\\\\thesis\\\\thesis\\\\train_reward_inverse_rl_w_sys_model\\\\norm_params.csv', header = None).to_numpy().T\n",
        "norm_params_actions = pd.read_csv('D:\\\\units\\\\thesis\\\\thesis\\\\train_reward_inverse_rl_w_sys_model\\\\norm_params_actions.csv', header = None).to_numpy().T\n",
        "\n",
        "##### TUNE MODEL WITH REWARD LOSS\n",
        "\n",
        "mean_rewards = []\n",
        "mean_loss = []\n",
        "mean_loss_rew = []\n",
        "\n",
        "# EACH BATCH HAS TRAJECTORIES\n",
        "EPISODES_FROM_DEMO = 3\n",
        "EPISODES_TO_PLAY = 5\n",
        "\n",
        "EPISODES_TO_TEST = 5\n",
        "\n",
        "BATCHES_DEMO = 200\n",
        "#ITERS_POLICY = 3\n",
        "\n",
        "\n",
        "# Retrain all params\n",
        "optimizer_policy = torch.optim.Adam(policy.parameters(), 1e-1, weight_decay=1e-2)\n",
        "optimizer_critic = torch.optim.Adam(critic.parameters(), 1e-1, weight_decay=1e-2)\n",
        "\n",
        "\n",
        "init_states = np.load(DATASETNPY, allow_pickle=True)\n",
        "demos = np.load(DATASET_DEMO, allow_pickle=True)\n",
        "\n",
        "criterionMSE = nn.MSELoss()\n",
        "\n",
        "t_step = 10\n",
        "t_max = 500\n",
        "\n",
        "gamma = 0.4\n",
        "############## OFFLINE RL - PRETRAIN CRITIC ####################\n",
        "loss_critic_lst = []\n",
        "for i in range(BATCHES_DEMO):\n",
        "    trajs_demo = [sample_demos(demos, l) for l in random.sample(range(400), EPISODES_FROM_DEMO)] \n",
        "    expected_rewards_lst = []\n",
        "    rewards_to_go_lst = []\n",
        "    for traj in trajs_demo:\n",
        "        states, actions, _, rewards = traj\n",
        "        step = 1\n",
        "        for t in range(0, t_max, t_step):\n",
        "            states_batch = states[:, 0:(step+1)*t_step]\n",
        "            actions_batch = actions[0:(step+1)*t_step]\n",
        "\n",
        "             # Skip first iter, since there is no \"previous value\"\n",
        "            if t > 4*t_step:\n",
        "\n",
        "                # When the scope is to get an action sequence only, use states for get_rewards()\n",
        "                rewards = get_rewards(states_batch[:, -t_step*2:-t_step], np.array(actions_batch), t_max)\n",
        "\n",
        "                # For Critic Update - TD ERROR\n",
        "                states_batch_np = states_batch.numpy()\n",
        "\n",
        "                input1 = states_batch_np[:,-1]  \n",
        "                for i in range(4):\n",
        "                    input1 = np.concatenate((input1, [np.max(states_batch_np[:,-5*(i+1):-5*(i)-1])]))\n",
        "                v_prime = critic(input1)\n",
        "\n",
        "                input2 = states_batch_np[:,-t_step]\n",
        "                for i in range(4):\n",
        "                    k = i+6\n",
        "                    input2 = np.concatenate((input2, [np.max(states_batch_np[:,-5*k:-5*(k-1)])]))\n",
        "                v = critic(input2)\n",
        "\n",
        "                # Critic\n",
        "\n",
        "                gamma_r = []\n",
        "                for i in range(t_step):\n",
        "                    expt = t_step + i -t_step\n",
        "                    gamma_r.append(gamma**expt)\n",
        "\n",
        "                target = torch.sum(torch.from_numpy(np.array(gamma_r))*rewards) + v_prime\n",
        "                predicted = v\n",
        "                # UPDATING THE VALUE NETWORK\n",
        "                loss_critic = criterionMSE(predicted.to(torch.float32), target.to(torch.float32))/1e3\n",
        "                \n",
        "                #optimizer_critic.zero_grad()\n",
        "                loss_critic.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(critic.parameters(), 0.01)\n",
        "                optimizer_critic.step()\n",
        "\n",
        "                print(f\"\\nLoss Critic: {loss_critic.item()}\\n\")\n",
        "                loss_critic_lst.append(loss_critic.item())\n",
        "\n",
        "            step += 1\n",
        "\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.title(f\"Critic Pre-train Loss\")\n",
        "plt.plot(loss_critic_lst)\n",
        "plt.grid()\n",
        "\n",
        "plt.savefig('Critic Pretrain Loss.png')\n",
        "plt.close()\n",
        "\n",
        "############## ON-POLICY #####################\n",
        "rewards_sum_plot = np.array((0))\n",
        "log_probs_for_actions_lst, cumulative_returns_lst = [], []\n",
        "#for i in range(ITERS_POLICY):\n",
        "[policy.train(sysmodel, policy, critic, init_states, l, t_step, t_max) for l in random.sample(range(400), EPISODES_TO_PLAY)]\n",
        "  \n",
        "trajs_test = [policy.rollout_policy(sysmodel, init_states, l, t_step, t_max) for l in random.sample(range(400), EPISODES_TO_TEST)]\n",
        "for traj in trajs_test:\n",
        "    states_history, states, actions, probs, rewards = traj\n",
        "\n",
        "    rewards_sum_plot = np.column_stack((rewards_sum_plot, np.array(rewards.sum())))\n",
        "\n",
        "    states_history_plot = np.array(states_history.squeeze())\n",
        "    states_plot = np.array(states.squeeze())\n",
        "    rewards_sum_plot2 = rewards_sum_plot.squeeze()\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.title(f\"Rewards Sum\")\n",
        "    plt.plot(rewards_sum_plot2[1:])\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.title(f\"Activations\")\n",
        "    plt.plot(actions[0:500])\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.title(f\"Temps Actual\")\n",
        "    plt.plot(states_history_plot[0:500].squeeze(), color='red')\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.title(f\"Temps Planned\")\n",
        "    plt.plot(states_plot[0:500].squeeze(), color = 'm')\n",
        "    plt.grid()\n",
        "\n",
        "    plt.savefig('Actor Critic Testing.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.6 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2276c0bad7efeb8779f7fdd76186059b8fbce5d8b401674f0514107ca347933d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
