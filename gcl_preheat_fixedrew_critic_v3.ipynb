{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import myModelinTF\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from torch.autograd import Variable \n",
        "\n",
        "from statistics import mean "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train firsts set size: 495\n"
          ]
        }
      ],
      "source": [
        "demo = np.load('dataset_train_firsts.npy', allow_pickle=True)\n",
        "print(\"Train firsts set size: \" + str(len(demo)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SeriesDataset(Dataset):\n",
        "\n",
        "    def __init__(self, file):\n",
        "        self.demo = np.load(file, allow_pickle=True)\n",
        "\n",
        "    #len(usd)\n",
        "    def __len__(self):\n",
        "        return len(self.demo)\n",
        "\n",
        "    # a_list[1] --> a__list.__getitem__(1)\n",
        "    def __getitem__(self, index):\n",
        "        run =self.demo[index]\n",
        "        states = np.array(run[0]).T\n",
        "        #states_init1 = states[1,:]\n",
        "        if states.size == 0:\n",
        "            print('no states in submatrix')\n",
        "\n",
        "        actions = np.array(run[1])[:, None]\n",
        "\n",
        "        action_count = np.array([actions[0][0], len(actions)])\n",
        "        #actions_counts_array_padded = np.pad(actions_counts_array, ((0,30-len(actions_counts_array)),(0,0)), 'constant').astype('int32') \n",
        "        #states_init = np.zeros((len(actions_counts_array_padded), 10)) + states_init1\n",
        "        #states_init = states[0:len(actions_counts_array),:]\n",
        "       \n",
        "        return states, action_count.astype('int32') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def permute_them(x):\n",
        "  return torch.permute(x, (1, 0, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fill_four_with_last(arr):\n",
        "    prev = np.arange(len(arr))\n",
        "    prev[arr == 4] = 0\n",
        "    prev = np.maximum.accumulate(prev)\n",
        "\n",
        "    # it can be that the first element is 4, so remove those\n",
        "    temp = arr[prev]\n",
        "    new_deleted = np.delete(temp, np.where(temp == 4))\n",
        "    count = len(temp) - len(new_deleted)\n",
        "    new = np.insert(new_deleted, 0, np.repeat(new_deleted[0], count), axis=0)\n",
        "    return new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PG(nn.Module):\n",
        "    def __init__(self, n_actions):\n",
        "        super().__init__()\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        # Policy takes input initial conditions and outputs planning route, open-loop control\n",
        "        self.actor_model = nn.Sequential(\n",
        "            nn.LSTM(input_size = 10, hidden_size = self.n_actions, num_layers = 1, dropout = 0.1,batch_first = True),\n",
        "        )\n",
        "\n",
        "        self.logprobs = []\n",
        "        self.state_values = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, states):\n",
        "        logits, (h_T, c_T)  = self.actor_model(states)\n",
        "        return logits[-1]\n",
        "    # Run agent in environment to create sample trajectories by generator\n",
        "    # The environment model is a seq2seq model\n",
        "    def generate_session(self, sysmodel, init_states, traj_idx,  t_max=50):\n",
        "        norm_params = pd.read_csv('D:\\\\units\\\\thesis\\\\thesis\\\\train_reward_inverse_rl_w_sys_model\\\\norm_params.csv', header = None).to_numpy().T\n",
        "\n",
        "        states, actions, actions_probs_policy, rewards = [], [], [], []\n",
        "        states_init_idx = init_states[traj_idx]\n",
        "\n",
        "        states = torch.FloatTensor(states_init_idx[0].T)\n",
        "        states = states.unsqueeze(0)\n",
        "        states_history = states[:,-1,:].unsqueeze(0)\n",
        "        list_actions_taken = []\n",
        "        actions_probs_policy_taken = []\n",
        "        action = 0\n",
        "        for t in range(0, t_max, 5):\n",
        "            logits = policy(states) # forward pass\n",
        "            logits = logits[-1]\n",
        "            probs = nn.functional.softmax(logits, -1) # get estimated actions for states\n",
        "            #log_probs = nn.functional.log_softmax(logits, -1)\n",
        "            \n",
        "            actions_probs_policy = np.squeeze(probs).clone().detach().numpy()\n",
        "            actions_probs_policy_rep = np.tile(actions_probs_policy,(5,1))\n",
        "            actions_probs_policy_taken.append(actions_probs_policy_rep)\n",
        "            # actions = []\n",
        "            # for prob in actions_probs_policy:                  \n",
        "            #     actions.append(np.random.choice(5,  p = prob))\n",
        "            action_prev = action\n",
        "            action = np.random.choice(5,  p = actions_probs_policy)\n",
        "            if action == 4:\n",
        "                action = action_prev\n",
        "\n",
        "            action_rep = np.repeat(action, 5)\n",
        "            list_actions_taken.append(action_rep)\n",
        "\n",
        "            #list_actions_filled = np.append(list_actions_filled, a)\n",
        "            \n",
        "            # Apply dict to go from 1,2,3,4 as action to [0,2400,0] [0,2400,1000]  [2300,0,0]  [2300,0,1000] \n",
        "            my_dict = {0:[0,2400,0], 1:[0,2400,1000], 2:[2300,0,0], 3:[2300,0,1000]}\n",
        "            #actions_array = np.zeros((500,3))  \n",
        "            actions_array = map(my_dict.get, np.array(list_actions_taken).flatten())\n",
        "            actions_array = np.array(list(actions_array))\n",
        "\n",
        "            actions_array_sysmodel = np.concatenate([np.arange(actions_array.shape[0])[:,None]+1+t*len(list_actions_taken),actions_array], axis=1) \n",
        "            actions_array_sysmodel = tf.expand_dims(actions_array_sysmodel, axis = 0, name=None)\n",
        "            \n",
        "            states = sysmodel(actions_array_sysmodel)\n",
        "\n",
        "            # Revert normalized values for states\n",
        "            states = states*norm_params[:,0]+norm_params[:,1]\n",
        "\n",
        "            states = torch.FloatTensor(np.array(states))\n",
        "            # More actions fed to the sysmodel, higher the accuracy, but the resulting states will not be the ones effectively experienced, so history is needed\n",
        "            # Rewards can be calculated on the set of states experienced with all actions at once, but then the scope is just to evaluate the set of actions in the whole episode, not step by step\n",
        "            states_history = torch.cat((states_history, states[:,-5:,:]), 1)\n",
        "\n",
        "            # if done:\n",
        "            #     break\n",
        "        \n",
        "        # print(actions_taken)\n",
        "        # actions_taken = np.array(actions_taken)\n",
        "        actions_probs_policy_taken = np.array(actions_probs_policy_taken).reshape(t_max, 5)\n",
        "        # states_all = sysmodel(actions_taken_reshape)\n",
        "        # states_all = states_new*norm_params[:,0]+norm_params[:,1]\n",
        "        rewards = get_rewards(states, np.array(list_actions_taken))\n",
        "        return states_history, states, np.array(list_actions_taken).flatten(), actions_probs_policy_taken, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.critic_model = nn.Sequential(\n",
        "            # nn.Linear(10, 32),\n",
        "            # nn.ReLU(inplace=False),\n",
        "            # nn.Linear(32, 1)\n",
        "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=5),\n",
        "            nn.LSTM(input_size = 8, hidden_size = 1, num_layers = 1, dropout = 0.1,batch_first = True),\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = torch.FloatTensor(state)\n",
        "        #state_value = self.critic_model(state)\n",
        "        logits, (state_value, c_T)  = self.critic_model(state)\n",
        "        return state_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "_DYTtGk7Fa1h"
      },
      "outputs": [],
      "source": [
        "def get_y_rewards(model,states, rewards,gamma=0.9):\n",
        "    states = states.numpy()\n",
        "    for idx_i in range(len(rewards)):\n",
        "        for idx in range(idx_i + 1, len(rewards)):\n",
        "            out2 = model(states[:,idx,:]).squeeze()\n",
        "            rewards[idx_i] += out2.detach().numpy()\n",
        "            continue\n",
        "    return rewards\n",
        "\n",
        "def get_critic_rewards(model,states, rewards,gamma=0.9):\n",
        "    G = np.zeros_like(rewards, dtype=float)\n",
        "    G[-1] = rewards[-1]\n",
        "    states = states.numpy()\n",
        "    for idx in range(len(rewards)):\n",
        "        out1 = model(states[:,idx,:])\n",
        "        #out2 = model(states[:,idx,:])\n",
        "        G[idx] = out1.detach().numpy()\n",
        "    return G\n",
        "\n",
        "def get_advantage_rewards(model,states, rewards,gamma=0.9):\n",
        "    G = np.zeros_like(rewards, dtype=float)\n",
        "    G[-1] = rewards[-1]\n",
        "    states = states.numpy()\n",
        "    for idx in range(len(rewards)-1):\n",
        "        out1 = model(states[:,idx,:])\n",
        "        out2 = model(states[:,idx+1,:])\n",
        "        G[idx] = rewards[idx] + out2.detach().numpy() - out1.detach().numpy()\n",
        "    return G\n",
        "\n",
        "def get_rewards(states, actions):\n",
        "    \n",
        "    states = states.numpy()\n",
        "    states = states.squeeze()\n",
        "    rewards = np.zeros((np.shape(states)[0],1))\n",
        "\n",
        "    actions = actions.flatten()\n",
        "    # print(actions)\n",
        "    # max_hotspot =  np.max(states)\n",
        "    # if max_hotspot < 300:\n",
        "    #     rewards[-1] = 100\n",
        "\n",
        "    # if np.where( 229 >= states[:,1].all() <= 231)[0] < 405 & np.where( 229 >= states[:,1].all() <= 231)[0] > 340:\n",
        "    #     rewards[-1] = rewards[-1] + 100\n",
        "\n",
        "        \n",
        "    for idx in range(np.shape(states)[1]):\n",
        "        idx = idx -1\n",
        "        # temps = np.array(states[idx])\n",
        "        # if temps.any() > 230:\n",
        "        #     rewards[idx] = rewards[idx] -30\n",
        "        \n",
        "        # subset = actions[idx-20:]\n",
        "        # if np.sum(np.diff(subset)) == 0:\n",
        "        #     rewards[idx] = rewards[idx] + 20\n",
        "\n",
        "        # subset = temps[idx-10:]\n",
        "        # if np.sum(np.diff(subset)) < 10:\n",
        "        #     rewards[idx] = rewards[idx] + 10\n",
        "\n",
        "        # Save power consumption\n",
        "        # if actions[idx] == 1 | actions[idx] == 3:\n",
        "        #     rewards[idx] = rewards[idx] -10\n",
        "\n",
        "        # Action 4 is encouraged , has the meaning to keep the previous action\n",
        "        if actions[idx] == 4:\n",
        "            rewards[idx] = rewards[idx] + 10\n",
        "\n",
        "        # temps_subset = np.array([states[idx, 1], states[idx-1, 1]])\n",
        "        # if np.diff(temps_subset) < 0:\n",
        "        #     rewards[idx] = rewards[idx] - 100\n",
        "\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def to_one_hot(y_tensor, ndims):\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IM1Ki1hL2U42",
        "outputId": "975040a9-996f-4879-cf39-9c3f89ebbc25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\units\\thesis\\thesis\\train_reward_inverse_rl_w_sys_model\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss Actor: 2.2145032348682974\n",
            "\n",
            "\n",
            "Loss Critic: 6673.67529296875\n",
            "\n",
            "\n",
            "Loss Actor: -1.2570560409721012\n",
            "\n",
            "\n",
            "Loss Critic: 39878.71875\n",
            "\n",
            "\n",
            "Loss Actor: 1.4505519860255671\n",
            "\n",
            "\n",
            "Loss Critic: 1.7746305465698242\n",
            "\n",
            "\n",
            "Loss Actor: 0.9308734716519314\n",
            "\n",
            "\n",
            "Loss Critic: 14256.2998046875\n",
            "\n",
            "\n",
            "Loss Actor: 1.827767665585331\n",
            "\n",
            "\n",
            "Loss Critic: 82680.5390625\n",
            "\n",
            "\n",
            "Loss Actor: 2.854206141407161\n",
            "\n",
            "\n",
            "Loss Critic: 2898.772705078125\n",
            "\n",
            "\n",
            "Loss Actor: 1.6882379050181306\n",
            "\n",
            "\n",
            "Loss Critic: 770.52685546875\n",
            "\n",
            "\n",
            "Loss Actor: -1.5089543960451044\n",
            "\n",
            "\n",
            "Loss Critic: 31466.80078125\n",
            "\n",
            "\n",
            "Loss Actor: 0.9978937702345483\n",
            "\n",
            "\n",
            "Loss Critic: 26851.2890625\n",
            "\n",
            "\n",
            "Loss Actor: 0.7623581033107945\n",
            "\n",
            "\n",
            "Loss Critic: 5492.3994140625\n",
            "\n",
            "\n",
            "Loss Actor: 0.6895609420682263\n",
            "\n",
            "\n",
            "Loss Critic: 179641.921875\n",
            "\n",
            "\n",
            "Loss Actor: 2.2463633891677355\n",
            "\n",
            "\n",
            "Loss Critic: 17701.22265625\n",
            "\n",
            "\n",
            "Loss Actor: 1.932627747589272\n",
            "\n",
            "\n",
            "Loss Critic: 62.10810852050781\n",
            "\n",
            "\n",
            "Loss Actor: 1.2713062558784782\n",
            "\n",
            "\n",
            "Loss Critic: 20445.517578125\n",
            "\n",
            "\n",
            "Loss Actor: 0.8550949646895661\n",
            "\n",
            "\n",
            "Loss Critic: 2097.982421875\n",
            "\n",
            "\n",
            "Loss Actor: -0.15105280675441923\n",
            "\n",
            "\n",
            "Loss Critic: 3293.133056640625\n",
            "\n",
            "\n",
            "Loss Actor: 0.9243100380613711\n",
            "\n",
            "\n",
            "Loss Critic: 362773.0\n",
            "\n",
            "\n",
            "Loss Actor: 1.4414145480032505\n",
            "\n",
            "\n",
            "Loss Critic: 1928.84423828125\n",
            "\n",
            "\n",
            "Loss Actor: 0.9902633199294828\n",
            "\n",
            "\n",
            "Loss Critic: 72724.3671875\n",
            "\n",
            "\n",
            "Loss Actor: 1.001988794718566\n",
            "\n",
            "\n",
            "Loss Critic: 21103.57421875\n",
            "\n",
            "\n",
            "Loss Actor: 1.3615124350520553\n",
            "\n",
            "\n",
            "Loss Critic: 40573.12109375\n",
            "\n",
            "\n",
            "Loss Actor: 1.0516598097466834\n",
            "\n",
            "\n",
            "Loss Critic: 4569.3720703125\n",
            "\n",
            "\n",
            "Loss Actor: -0.009721763599017885\n",
            "\n",
            "\n",
            "Loss Critic: 166042.0\n",
            "\n",
            "\n",
            "Loss Actor: 0.08762839159388636\n",
            "\n",
            "\n",
            "Loss Critic: 325833.9375\n",
            "\n",
            "\n",
            "Loss Actor: 2.2080207330616806\n",
            "\n",
            "\n",
            "Loss Critic: 1126.7320556640625\n",
            "\n",
            "\n",
            "Loss Actor: 0.11744344072390689\n",
            "\n",
            "\n",
            "Loss Critic: 13571.5283203125\n",
            "\n",
            "\n",
            "Loss Actor: 0.761386526934976\n",
            "\n",
            "\n",
            "Loss Critic: 16.0631160736084\n",
            "\n",
            "\n",
            "Loss Actor: 0.7037609042995423\n",
            "\n",
            "\n",
            "Loss Critic: 291637.625\n",
            "\n",
            "\n",
            "Loss Actor: 2.3205109673352147\n",
            "\n",
            "\n",
            "Loss Critic: 214340.828125\n",
            "\n",
            "\n",
            "Loss Actor: 2.2105630133658414\n",
            "\n",
            "\n",
            "Loss Critic: 542.6673583984375\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# DEVICE\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "# ENV SETUP\n",
        "sysmodel = myModelinTF.load_model()\n",
        "n_actions = 5\n",
        "\n",
        "# INITILIZING POLICY AND REWARD FUNCTION\n",
        "policy = PG(n_actions)\n",
        "policy.to(DEVICE)\n",
        "\n",
        "critic = Critic()\n",
        "critic.to(DEVICE)\n",
        "\n",
        "norm_params = pd.read_csv('D:\\\\units\\\\thesis\\\\thesis\\\\train_reward_inverse_rl_w_sys_model\\\\norm_params.csv', header = None).to_numpy().T\n",
        "\n",
        "\n",
        "# LOADING EXPERT/DEMO SAMPLES\n",
        "DATASETNPY = \"dataset_train_firsts.npy\"\n",
        "\n",
        "dataset_inits = SeriesDataset(DATASETNPY)\n",
        "\n",
        "##### TUNE MODEL WITH REWARD LOSS\n",
        "\n",
        "mean_rewards = []\n",
        "mean_loss = []\n",
        "mean_loss_rew = []\n",
        "EPISODES_TO_PLAY = 3\n",
        "sample_trajs = []\n",
        "\n",
        "return_list, sum_of_loss_list = [], []\n",
        "\n",
        "# Retrain all params\n",
        "optimizer_policy = torch.optim.Adam(policy.parameters(), 1e-1, weight_decay=1e-2)\n",
        "optimizer_critic = torch.optim.Adam(critic.parameters(), 1e-1, weight_decay=1e-2)\n",
        "\n",
        "init_states = np.load(DATASETNPY, allow_pickle=True)\n",
        "\n",
        "criterion2 = nn.MSELoss()\n",
        "\n",
        "#Epochs\n",
        "for i in range(10):\n",
        "    trajs = [policy.generate_session(sysmodel, init_states, l) for l in range(EPISODES_TO_PLAY)]\n",
        "\n",
        "    # Episodes\n",
        "    for traj in trajs:\n",
        "        states_history, states, actions, probs, rewards = traj\n",
        "\n",
        "        cumulative_returns_np = np.array(get_advantage_rewards(critic,states,rewards, 0.7)).squeeze()\n",
        "        cumulative_returns = torch.tensor(cumulative_returns_np, dtype=torch.float32)\n",
        "        #cumulative_returns = (cumulative_returns - torch.mean(cumulative_returns)) # torch.mean\n",
        "\n",
        "        actions_tensor = torch.from_numpy(actions)\n",
        "        probs_tensor = torch.from_numpy(np.array(probs))\n",
        "\n",
        "        log_probs_for_actions = torch.log(torch.sum(\n",
        "            probs_tensor * to_one_hot(actions_tensor, 5), dim =1))\n",
        "    \n",
        "        loss_policy = -log_probs_for_actions*cumulative_returns_np\n",
        "        # UPDATING THE POLICY NETWORK\n",
        "        optimizer_policy.zero_grad()\n",
        "        loss_policy.requires_grad = True\n",
        "        loss_policy.sum().backward()\n",
        "        torch.nn.utils.clip_grad_norm_(policy.parameters(), 0.01)\n",
        "        optimizer_policy.step()\n",
        "\n",
        "        loss_critic = criterion2(torch.from_numpy(np.array(get_critic_rewards(critic,states,rewards, 0.7).sum())).to(torch.float32), \\\n",
        "                                 torch.from_numpy(np.array(get_y_rewards(critic, states, rewards, 0.7).sum())).to(torch.float32))\n",
        "        # UPDATING THE CRITIC NETWORK\n",
        "        optimizer_critic.zero_grad()\n",
        "        loss_critic.requires_grad = True\n",
        "        loss_critic.backward()\n",
        "        optimizer_critic.step()\n",
        "\n",
        "        #returns = sum(rewards)\n",
        "        sum_of_loss = loss_policy.sum().detach().numpy()\n",
        "        print(f\"\\nLoss Actor: {sum_of_loss}\\n\")\n",
        "        print(f\"\\nLoss Critic: {loss_critic}\\n\")\n",
        "        #print(f\"\\nCumulative return: {cumulative_returns}\\n\")\n",
        "        #return_list.append(returns)\n",
        "        sum_of_loss_list.append(sum_of_loss)\n",
        "        states_plot = np.array(states_history.squeeze())\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.title(f\"Loss per {EPISODES_TO_PLAY} episodes\")\n",
        "    plt.plot(sum_of_loss_list)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.title(f\"Rewards\")\n",
        "    plt.plot(rewards)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.title(f\"Activations\")\n",
        "    plt.plot(actions[0:50])\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.title(f\"Temperatures\")\n",
        "    plt.plot(states_plot[0:50,:])\n",
        "    plt.grid()\n",
        "\n",
        "    # plt.show()\n",
        "    plt.savefig('GCL_learning_curve.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.6 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2276c0bad7efeb8779f7fdd76186059b8fbce5d8b401674f0514107ca347933d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
