{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import myModelinTF\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from torch.autograd import Variable \n",
        "\n",
        "from statistics import mean "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train firsts set size: 495\n"
          ]
        }
      ],
      "source": [
        "demo = np.load('dataset_train_firsts.npy', allow_pickle=True)\n",
        "print(\"Train firsts set size: \" + str(len(demo)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SeriesDataset(Dataset):\n",
        "\n",
        "    def __init__(self, file):\n",
        "        self.demo = np.load(file, allow_pickle=True)\n",
        "\n",
        "    #len(usd)\n",
        "    def __len__(self):\n",
        "        return len(self.demo)\n",
        "\n",
        "    # a_list[1] --> a__list.__getitem__(1)\n",
        "    def __getitem__(self, index):\n",
        "        run =self.demo[index]\n",
        "        states = np.array(run[0]).T\n",
        "        #states_init1 = states[1,:]\n",
        "        if states.size == 0:\n",
        "            print('no states in submatrix')\n",
        "\n",
        "        actions = np.array(run[1])[:, None]\n",
        "\n",
        "        action_count = np.array([actions[0][0], len(actions)])\n",
        "        #actions_counts_array_padded = np.pad(actions_counts_array, ((0,30-len(actions_counts_array)),(0,0)), 'constant').astype('int32') \n",
        "        #states_init = np.zeros((len(actions_counts_array_padded), 10)) + states_init1\n",
        "        #states_init = states[0:len(actions_counts_array),:]\n",
        "       \n",
        "        return states, action_count.astype('int32') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def permute_them(x):\n",
        "  return torch.permute(x, (1, 0, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fill_four_with_last(arr):\n",
        "    prev = np.arange(len(arr))\n",
        "    prev[arr == 4] = 0\n",
        "    prev = np.maximum.accumulate(prev)\n",
        "\n",
        "    # it can be that the first element is 4, so remove those\n",
        "    temp = arr[prev]\n",
        "    new_deleted = np.delete(temp, np.where(temp == 4))\n",
        "    count = len(temp) - len(new_deleted)\n",
        "    new = np.insert(new_deleted, 0, np.repeat(new_deleted[0], count), axis=0)\n",
        "    return new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PG(nn.Module):\n",
        "    def __init__(self, n_actions):\n",
        "        super().__init__()\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        # Policy takes input initial conditions and outputs planning route, open-loop control\n",
        "        self.actor_model = nn.Sequential(\n",
        "            nn.LSTM(input_size = 10, hidden_size = self.n_actions, num_layers = 1, dropout = 0.1,batch_first = True),\n",
        "        )\n",
        "\n",
        "        self.logprobs = []\n",
        "        self.state_values = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, states):\n",
        "        logits, (h_T, c_T)  = self.actor_model(states)\n",
        "        return logits[-1]\n",
        "    # Run agent in environment to create sample trajectories by generator\n",
        "    # The environment model is a seq2seq model\n",
        "    def generate_session(self, sysmodel, init_states, traj_idx,  t_max=50):\n",
        "        norm_params = pd.read_csv('D:\\\\units\\\\thesis\\\\thesis\\\\train_reward_inverse_rl_w_sys_model\\\\norm_params.csv', header = None).to_numpy().T\n",
        "\n",
        "        states, actions, actions_probs_policy, rewards = [], [], [], []\n",
        "        states_init_idx = init_states[traj_idx]\n",
        "\n",
        "        states = torch.FloatTensor(states_init_idx[0].T)\n",
        "        states = states.unsqueeze(0)\n",
        "        actions_taken = []\n",
        "        list_actions_taken = []\n",
        "        actions_probs_policy_taken = []\n",
        "        list_actions_filled = []\n",
        "        for t in range(t_max):\n",
        "            logits = policy(states) # forward pass\n",
        "            logits = logits[-1]\n",
        "            probs = nn.functional.softmax(logits, -1) # get estimated actions for states\n",
        "            #log_probs = nn.functional.log_softmax(logits, -1)\n",
        "            \n",
        "            actions_probs_policy = np.squeeze(probs).clone().detach().numpy()\n",
        "            actions_probs_policy_taken.append(actions_probs_policy)\n",
        "            # actions = []\n",
        "            # for prob in actions_probs_policy:                  \n",
        "            #     actions.append(np.random.choice(5,  p = prob))\n",
        "            \n",
        "            action = np.random.choice(5,  p = actions_probs_policy)\n",
        "            if t==0 and action == 4:\n",
        "                action = 0\n",
        "\n",
        "            list_actions_taken.append(action)\n",
        "            # actions was an array of individual type of actions to take, but also 4 meaning \"keep\"\n",
        "\n",
        "            a_filled = fill_four_with_last(np.array(list_actions_taken)).tolist()\n",
        "            #list_actions_filled = np.append(list_actions_filled, a)\n",
        "            \n",
        "            # Apply dict to go from 1,2,3,4 as action to [0,2400,0] [0,2400,1000]  [2300,0,0]  [2300,0,1000] \n",
        "            my_dict = {0:[0,2400,0], 1:[0,2400,1000], 2:[2300,0,0], 3:[2300,0,1000]}\n",
        "            #actions_array = np.zeros((500,3))  \n",
        "            actions_array = map(my_dict.get, a_filled)\n",
        "            actions_array = np.array(list(actions_array))\n",
        "            actions_array_sysmodel = np.concatenate([np.arange(actions_array.shape[0])[:,None]+1+t*len(a_filled),actions_array], axis=1) \n",
        "            actions_array_sysmodel = tf.expand_dims(actions_array_sysmodel, axis = 0, name=None)\n",
        "            \n",
        "            states_prev = states\n",
        "            states = sysmodel(actions_array_sysmodel)\n",
        "\n",
        "            # Revert normalized values for states\n",
        "            states = states*norm_params[:,0]+norm_params[:,1]\n",
        "            states_new = states\n",
        "\n",
        "            states = torch.FloatTensor(np.array(states))\n",
        "            states = torch.cat((states_prev, states), 1)\n",
        "\n",
        "            # if done:\n",
        "            #     break\n",
        "        \n",
        "        # print(actions_taken)\n",
        "        # actions_taken = np.array(actions_taken)\n",
        "        # actions_taken_reshape = actions_taken.reshape(1,t_max*states.size(dim=1), 4)\n",
        "        # states_all = sysmodel(actions_taken_reshape)\n",
        "        # states_all = states_new*norm_params[:,0]+norm_params[:,1]\n",
        "        rewards = get_rewards(states_new, np.array(list_actions_taken))\n",
        "        return states_new, list_actions_taken, np.array(actions_probs_policy_taken), rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.critic_model = nn.Sequential(\n",
        "            # nn.Linear(10, 32),\n",
        "            # nn.ReLU(inplace=False),\n",
        "            # nn.Linear(32, 1)\n",
        "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=5),\n",
        "            nn.LSTM(input_size = 8, hidden_size = 1, num_layers = 1, dropout = 0.1,batch_first = True),\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = torch.FloatTensor(state)\n",
        "        #state_value = self.critic_model(state)\n",
        "        logits, (state_value, c_T)  = self.critic_model(state)\n",
        "        return state_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_DYTtGk7Fa1h"
      },
      "outputs": [],
      "source": [
        "def get_expected_rewards(rewards,gamma=0.9):\n",
        "    G = np.zeros_like(rewards, dtype=float)\n",
        "    G[-1] = rewards[-1]\n",
        "    \n",
        "    for idx in range(-2, -len(rewards)-1, -1):\n",
        "        G[idx] = rewards[idx] + gamma * G[idx+1]\n",
        "    return G\n",
        "\n",
        "def get_critic_rewards(model,states, rewards,gamma=0.9):\n",
        "    G = np.zeros_like(rewards, dtype=float)\n",
        "    G[-1] = rewards[-1]\n",
        "    states = states.numpy()\n",
        "    for idx in range(-2, -len(rewards)+1, -1):\n",
        "        out1 = model(states[:,idx-1,:])\n",
        "        out2 = model(states[:,idx,:])\n",
        "        G[idx] = rewards[idx] + 0.9*out1.detach().numpy() - out2.detach().numpy()\n",
        "    return G\n",
        "\n",
        "def get_rewards(states, actions):\n",
        "    \n",
        "    states = states.numpy()\n",
        "    states = states.squeeze()\n",
        "    rewards = np.zeros((np.shape(states)[0],1))\n",
        "\n",
        "    actions = actions.flatten()\n",
        "    # print(actions)\n",
        "    # max_hotspot =  np.max(states)\n",
        "    # if max_hotspot < 300:\n",
        "    #     rewards[-1] = 100\n",
        "\n",
        "    # if np.where( 229 >= states[:,1].all() <= 231)[0] < 405 & np.where( 229 >= states[:,1].all() <= 231)[0] > 340:\n",
        "    #     rewards[-1] = rewards[-1] + 100\n",
        "\n",
        "        \n",
        "    for idx in range(np.shape(states)[1]):\n",
        "        idx = idx -1\n",
        "        # temps = np.array(states[idx])\n",
        "        # if temps.any() > 230:\n",
        "        #     rewards[idx] = rewards[idx] -30\n",
        "        \n",
        "        # subset = actions[idx-20:]\n",
        "        # if np.sum(np.diff(subset)) == 0:\n",
        "        #     rewards[idx] = rewards[idx] + 20\n",
        "\n",
        "        # subset = temps[idx-10:]\n",
        "        # if np.sum(np.diff(subset)) < 10:\n",
        "        #     rewards[idx] = rewards[idx] + 10\n",
        "\n",
        "        # Save power consumption\n",
        "        # if actions[idx] == 1 | actions[idx] == 3:\n",
        "        #     rewards[idx] = rewards[idx] -10\n",
        "\n",
        "        # Action 4 is encouraged , has the meaning to keep the previous action\n",
        "        if actions[idx] == 4:\n",
        "            rewards[idx] = rewards[idx] + 10\n",
        "\n",
        "        # temps_subset = np.array([states[idx, 1], states[idx-1, 1]])\n",
        "        # if np.diff(temps_subset) < 0:\n",
        "        #     rewards[idx] = rewards[idx] - 100\n",
        "\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def to_one_hot(y_tensor, ndims):\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IM1Ki1hL2U42",
        "outputId": "975040a9-996f-4879-cf39-9c3f89ebbc25"
      },
      "outputs": [],
      "source": [
        "# DEVICE\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "# ENV SETUP\n",
        "sysmodel = myModelinTF.load_model()\n",
        "n_actions = 5\n",
        "\n",
        "# INITILIZING POLICY AND REWARD FUNCTION\n",
        "policy = PG(n_actions)\n",
        "policy.to(DEVICE)\n",
        "\n",
        "critic = Critic()\n",
        "critic.to(DEVICE)\n",
        "\n",
        "norm_params = pd.read_csv('D:\\\\units\\\\thesis\\\\thesis\\\\train_reward_inverse_rl_w_sys_model\\\\norm_params.csv', header = None).to_numpy().T\n",
        "\n",
        "\n",
        "# LOADING EXPERT/DEMO SAMPLES\n",
        "DATASETNPY = \"dataset_train_firsts.npy\"\n",
        "\n",
        "dataset_inits = SeriesDataset(DATASETNPY)\n",
        "\n",
        "##### TUNE MODEL WITH REWARD LOSS\n",
        "\n",
        "mean_rewards = []\n",
        "mean_loss = []\n",
        "mean_loss_rew = []\n",
        "EPISODES_TO_PLAY = 70\n",
        "sample_trajs = []\n",
        "\n",
        "return_list, sum_of_loss_list = [], []\n",
        "\n",
        "# Retrain all params\n",
        "optimizer_policy = torch.optim.Adam(policy.parameters(), 1e-1, weight_decay=1e-2)\n",
        "optimizer_critic = torch.optim.Adam(critic.parameters(), 1e-1, weight_decay=1e-2)\n",
        "\n",
        "init_states = np.load(DATASETNPY, allow_pickle=True)\n",
        "\n",
        "criterion2 = nn.MSELoss()\n",
        "\n",
        "#Epochs\n",
        "for i in range(10):\n",
        "    trajs = [policy.generate_session(sysmodel, init_states, l) for l in range(EPISODES_TO_PLAY)]\n",
        "\n",
        "    # Episodes\n",
        "    for traj in trajs:\n",
        "        states, actions, probs, rewards = traj\n",
        "        \n",
        "        cumulative_returns_np = np.array(get_critic_rewards(critic,states,rewards, 0.7)).squeeze()\n",
        "        cumulative_returns = torch.tensor(cumulative_returns_np, dtype=torch.float32)\n",
        "        #cumulative_returns = (cumulative_returns - torch.mean(cumulative_returns)) # torch.mean\n",
        "\n",
        "        actions_tensor = torch.from_numpy(np.array(actions))\n",
        "        probs_tensor = torch.from_numpy(np.array(probs))\n",
        "\n",
        "        log_probs_for_actions = torch.sum(\n",
        "            probs_tensor * to_one_hot(actions_tensor, 5), dim=1)\n",
        "    \n",
        "        loss_policy = -log_probs_for_actions*cumulative_returns_np\n",
        "        # UPDATING THE POLICY NETWORK\n",
        "        optimizer_policy.zero_grad()\n",
        "        loss_policy.requires_grad = True\n",
        "        loss_policy.sum().backward()\n",
        "        torch.nn.utils.clip_grad_norm_(policy.parameters(), 0.01)\n",
        "        optimizer_policy.step()\n",
        "\n",
        "        loss_critic = criterion2(torch.from_numpy(np.array(get_critic_rewards(critic,states,rewards, 0.7).sum())).to(torch.float32), \\\n",
        "                                 torch.from_numpy(np.array(get_expected_rewards(rewards, 0.7).sum())).to(torch.float32))\n",
        "        # UPDATING THE CRITIC NETWORK\n",
        "        optimizer_critic.zero_grad()\n",
        "        loss_critic.requires_grad = True\n",
        "        loss_critic.backward()\n",
        "        optimizer_critic.step()\n",
        "\n",
        "        #returns = sum(rewards)\n",
        "        sum_of_loss = loss_policy.sum().detach().numpy()\n",
        "        print(f\"\\nLoss Actor: {sum_of_loss}\\n\")\n",
        "        print(f\"\\nLoss Critic: {loss_critic}\\n\")\n",
        "        #print(f\"\\nCumulative return: {cumulative_returns}\\n\")\n",
        "        #return_list.append(returns)\n",
        "        sum_of_loss_list.append(sum_of_loss)\n",
        "        states_plot = np.squeeze(states)\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.title(f\"Loss per {EPISODES_TO_PLAY} episodes\")\n",
        "    plt.plot(sum_of_loss_list)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.title(f\"Rewards\")\n",
        "    plt.plot(rewards)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.title(f\"Activations\")\n",
        "    plt.plot(actions[0:50])\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.title(f\"Temperatures\")\n",
        "    plt.plot(states_plot[0:50,:])\n",
        "    plt.grid()\n",
        "\n",
        "    # plt.show()\n",
        "    plt.savefig('GCL_learning_curve.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.6 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2276c0bad7efeb8779f7fdd76186059b8fbce5d8b401674f0514107ca347933d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
