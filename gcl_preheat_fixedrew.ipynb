{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import myModelinTF\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from torch.autograd import Variable "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SeriesDataset(Dataset):\n",
        "\n",
        "    def __init__(self, file):\n",
        "        self.demo = np.load(file, allow_pickle=True)\n",
        "\n",
        "    #len(usd)\n",
        "    def __len__(self):\n",
        "        return len(self.demo)\n",
        "\n",
        "    # a_list[1] --> a__list.__getitem__(1)\n",
        "    def __getitem__(self, index):\n",
        "        run =self.demo[index]\n",
        "\n",
        "        states = np.array(run[0]).T\n",
        "        states_init1 = states[1,:]\n",
        "        states_init = np.zeros((500, 10)) + states_init1\n",
        "        actions = np.array(run[1])[:, None]\n",
        "        return states_init, actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "def permute_them(x):\n",
        "  return torch.permute(x, (1, 0, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "\n",
        "    # A data tuple has the form:\n",
        "\n",
        "    tensors, targets = [], []\n",
        "\n",
        "    # Gather in lists, and encode labels as indices\n",
        "    for states_init, actions in batch:\n",
        "        tensors += [torch.tensor(states_init).type(torch.FloatTensor)]\n",
        "        targets += [torch.tensor(actions).type(torch.LongTensor)]\n",
        "\n",
        "    # Group the list of tensors into a batched tensor\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    return tensors, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PG(nn.Module):\n",
        "    def __init__(self, state_shape, n_actions):\n",
        "        super().__init__()\n",
        "        self.state_shape = state_shape\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        # Policy takes input initial conditions and outputs planning route, open-loop control\n",
        "        self.model = nn.Sequential(\n",
        "            nn.LSTM(input_size = 10, hidden_size = 4, num_layers = 1, dropout = 0.2,batch_first = True)\n",
        "        )\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), 1e-3)\n",
        "\n",
        "    def forward(self, states):\n",
        "        #states = torch.FloatTensor(states)\n",
        "        logits, (h_T, c_T)  = self.model(states)\n",
        "        return logits\n",
        "    \n",
        "    def predict_probs(self, states):\n",
        "        states = torch.FloatTensor(states)\n",
        "        logits, (h_T, c_T) = self.model(states)\n",
        "        probs = F.softmax(logits, dim = -1).detach().numpy()\n",
        "        return probs\n",
        "    \n",
        "    # Run agent in environment to create sample trajectories by generator\n",
        "    # The environment model is a seq2seq model\n",
        "    def generate_session(self, sysmodel, t_max=1000):\n",
        "        norm_params = pd.read_csv('D:\\\\units\\\\thesis\\\\thesis\\\\train_reward_inverse_rl_w_sys_model\\\\norm_params.csv', header = None).to_numpy().T\n",
        "\n",
        "        states, actions, traj_probs, rewards = [], [], [], []\n",
        "        states_init = np.zeros((500,1, 10)) + np.random.randint(20, 34, size=(10)) \n",
        "        actions_probs_policy = self.predict_probs(states_init)\n",
        "        actions = []\n",
        "        actions_probs_policy = np.squeeze(actions_probs_policy)     \n",
        "        for prob in actions_probs_policy:                  \n",
        "            actions.append(np.random.choice(self.n_actions,  p = prob))\n",
        " \n",
        "        # Apply dict to go from 1,2,3,4 as action to [0,2400,0] [0,2400,1000]  [2300,0,0]  [2300,0,1000] \n",
        "        my_dict = {0:[0,2400,0], 1:[0,2400,1000], 2:[2300,0,0], 3:[2300,0,1000]}\n",
        "        actions_array = np.zeros((500,3))  \n",
        "        actions_array = map(my_dict.get, actions)\n",
        "\n",
        "        actions_array = np.array(list(actions_array))\n",
        "        actions_array_sysmodel = np.concatenate([np.arange(actions_array.shape[0])[:,None]+1,actions_array], axis=1) \n",
        "        actions_array_sysmodel = tf.expand_dims(actions_array_sysmodel, axis = 0, name=None)\n",
        "\n",
        "        states = sysmodel(actions_array_sysmodel)\n",
        "        states = states*norm_params[:,0]+norm_params[:,1]\n",
        "        \n",
        "        return states, actions, actions_probs_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, tepoch, log_interval):\n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "    batch_idx = 0\n",
        "    \n",
        "    for data, target in train_loader: #tqdm_notebook(train_loader, total = len(train_loader)):\n",
        "        data = torch.stack(data)\n",
        "        \n",
        "        output = model(data)\n",
        "        target = to_one_hot(target, 4)\n",
        "        output = torch.flatten(output, start_dim = 0, end_dim = 1)\n",
        "        \n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        output_np = output.detach().numpy().tolist()\n",
        "        target_np = target.detach().numpy().tolist()\n",
        "        correct = 0\n",
        "        for (output, target) in zip(output_np, target_np):           \n",
        "            if output.index(max(output)) == target.index(max(target)):\n",
        "                correct +=1\n",
        "        accuracy = correct / 500 * BATCH_SIZE\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print training stats\n",
        "        #if batch_idx % log_interval == 0:\n",
        "        print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\\tAccuracy of batch: {accuracy:.6f}\")\n",
        "        # record loss\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "\n",
        "        batch_idx += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(model, epoch):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "\n",
        "    for data, target in test_loader:\n",
        "        data = torch.stack(data)\n",
        "\n",
        "        output = model(data)\n",
        "        target = to_one_hot(target, 4)\n",
        "        output = torch.flatten(output, start_dim = 0, end_dim = 1)\n",
        "        \n",
        "        output_np = output.detach().numpy().tolist()\n",
        "        target_np = target.detach().numpy().tolist()\n",
        "        correct = 0\n",
        "        for (output, target) in zip(output_np, target_np):           \n",
        "            if output.index(max(output)) == target.index(max(target)):\n",
        "                correct +=1\n",
        "\n",
        "        accuracy = correct / 500 * BATCH_SIZE\n",
        "\n",
        "    print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {accuracy}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "_DYTtGk7Fa1h"
      },
      "outputs": [],
      "source": [
        "def number_of_correct(pred, target):\n",
        "    # count number of correct predictions\n",
        "    return pred.squeeze().eq(target).sum().item()\n",
        "\n",
        "def get_likely_index(tensor):\n",
        "    # find most likely label index for each element in the batch\n",
        "    return tensor.argmax(dim=-1)\n",
        "\n",
        "def compute_error(x, y):\n",
        "    Ps = torch.norm(x)\n",
        "    Pn = torch.norm(x-y)\n",
        "    return 20*torch.log10(Ps/Pn)\n",
        "\n",
        "def get_expected_rewards(rewards,gamma=0.9):\n",
        "    G = np.zeros_like(rewards, dtype=float)\n",
        "    G[-1] = rewards[-1]\n",
        "    for idx in range(-2, -len(rewards)-1, -1):\n",
        "        G[idx] = rewards[idx] + gamma * G[idx+1]\n",
        "    return G\n",
        "\n",
        "def get_rewards(states, actions):\n",
        "    rewards = np.zeros((500,1))\n",
        "\n",
        "    states = states.numpy()\n",
        "\n",
        "    max_hotspot =  np.max(states)\n",
        "    if max_hotspot < 250:\n",
        "        rewards[-1] = 50\n",
        "\n",
        "    if np.where(states[:,1] == 230)[0] < 405 & np.where(states[:,1] == 230)[0] > 340:\n",
        "        rewards[-1] = rewards[-1] + 40\n",
        "        \n",
        "    for idx in range(len(states)):\n",
        "        temps = np.array(states[idx])\n",
        "        if temps.any() > 250:\n",
        "            rewards[idx] = rewards[idx] -20\n",
        "        \n",
        "        subset = actions[idx-10:]\n",
        "        if np.sum(np.diff(subset)) == 0:\n",
        "            rewards[idx] = rewards[idx] + 20\n",
        "\n",
        "        if actions[idx] == 1 | actions[idx] == 3:\n",
        "            rewards[idx] = rewards[idx] -10\n",
        "\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def to_one_hot(y_tensor, ndims):\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONVERTS TRAJ LIST TO STEP LIST\n",
        "def preprocess_traj(traj_list, step_list):\n",
        "    step_list = step_list.tolist()\n",
        "    for traj in traj_list:\n",
        "        states = np.array(traj[0])\n",
        "        probs = np.ones((500,4))\n",
        "        states = np.transpose(traj[0])\n",
        "        actions = np.array(traj[1])[:, None]\n",
        "\n",
        "        x = np.concatenate((states, actions, probs), axis=1) \n",
        "        step_list.extend(x)\n",
        "    return np.array(step_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IM1Ki1hL2U42",
        "outputId": "975040a9-996f-4879-cf39-9c3f89ebbc25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\units\\thesis\\thesis\\train_reward_inverse_rl_w_sys_model\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set size: 495\n",
            "Train Epoch: 1 [0/495 (0%)]\tLoss: 1.923454\tAccuracy of batch: 287.552000\n",
            "Train Epoch: 1 [32/495 (6%)]\tLoss: 1.692882\tAccuracy of batch: 320.832000\n",
            "Train Epoch: 1 [64/495 (12%)]\tLoss: 1.536738\tAccuracy of batch: 289.664000\n",
            "Train Epoch: 1 [96/495 (19%)]\tLoss: 1.367030\tAccuracy of batch: 319.232000\n",
            "Train Epoch: 1 [128/495 (25%)]\tLoss: 1.242898\tAccuracy of batch: 297.280000\n",
            "Train Epoch: 1 [160/495 (31%)]\tLoss: 1.217640\tAccuracy of batch: 293.248000\n",
            "Train Epoch: 1 [192/495 (38%)]\tLoss: 1.216680\tAccuracy of batch: 296.256000\n",
            "Train Epoch: 1 [224/495 (44%)]\tLoss: 1.218072\tAccuracy of batch: 300.224000\n",
            "Train Epoch: 1 [256/495 (50%)]\tLoss: 1.216443\tAccuracy of batch: 300.672000\n",
            "Train Epoch: 1 [288/495 (56%)]\tLoss: 1.218071\tAccuracy of batch: 305.728000\n",
            "Train Epoch: 1 [320/495 (62%)]\tLoss: 1.218071\tAccuracy of batch: 315.456000\n",
            "Train Epoch: 1 [352/495 (69%)]\tLoss: 1.219000\tAccuracy of batch: 311.104000\n",
            "Train Epoch: 1 [384/495 (75%)]\tLoss: 1.216908\tAccuracy of batch: 308.736000\n",
            "Train Epoch: 1 [416/495 (81%)]\tLoss: 1.217373\tAccuracy of batch: 308.160000\n",
            "Train Epoch: 1 [448/495 (88%)]\tLoss: 1.218070\tAccuracy of batch: 303.360000\n",
            "Train Epoch: 1 [225/495 (94%)]\tLoss: 1.217822\tAccuracy of batch: 138.112000\n",
            "\n",
            "Test Epoch: 1\tAccuracy: 44.736/5 (13980%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\units\\thesis\\thesis\\train_reward_inverse_rl_w_sys_model\\.venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 2 [0/495 (0%)]\tLoss: 1.217838\tAccuracy of batch: 289.408000\n",
            "Train Epoch: 2 [32/495 (6%)]\tLoss: 1.218535\tAccuracy of batch: 301.696000\n",
            "Train Epoch: 2 [64/495 (12%)]\tLoss: 1.217373\tAccuracy of batch: 290.688000\n",
            "Train Epoch: 2 [96/495 (19%)]\tLoss: 1.218070\tAccuracy of batch: 486.528000\n",
            "Train Epoch: 2 [128/495 (25%)]\tLoss: 1.217838\tAccuracy of batch: 470.016000\n"
          ]
        }
      ],
      "source": [
        "# DEVICE\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "# ENV SETUP\n",
        "sysmodel = myModelinTF.load_model()\n",
        "n_actions = 4\n",
        "state = np.zeros((500,10)) + 25\n",
        "state_shape = state.shape\n",
        "\n",
        "# INITILIZING POLICY AND REWARD FUNCTION\n",
        "policy = PG(state_shape, n_actions)\n",
        "policy.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(policy.parameters(), 1e-2, weight_decay=1e-4)\n",
        "\n",
        "mean_rewards = []\n",
        "mean_loss = []\n",
        "mean_loss_rew = []\n",
        "EPISODES_TO_PLAY = 5000\n",
        "sample_trajs = []\n",
        "\n",
        "return_list, sum_of_loss_list = [], []\n",
        "norm_params = pd.read_csv('D:\\\\units\\\\thesis\\\\thesis\\\\train_reward_inverse_rl_w_sys_model\\\\norm_params.csv', header = None).to_numpy().T\n",
        "\n",
        "# LOADING EXPERT/DEMO SAMPLES\n",
        "\n",
        "# PRE-TRAIN WITH EXISTING DATA THE POLICY\n",
        "\n",
        "LOG_INTERVAL = 2\n",
        "N_EPOCH = 200\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(policy_optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "DATASETNPY_TRAIN = \"dataset_train.npy\"\n",
        "DATASETNPY_TEST = \"dataset_test.npy\"\n",
        "\n",
        "train_set = SeriesDataset(DATASETNPY_TRAIN)\n",
        "test_set = SeriesDataset(DATASETNPY_TEST)\n",
        "\n",
        "print(\"Train set size: \" + str(len(train_set)))\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_set,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        test_set,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "\n",
        "for epoch in range(1, N_EPOCH + 1):\n",
        "    train(policy, epoch, LOG_INTERVAL)\n",
        "    test(policy, epoch)\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "for i in range(EPISODES_TO_PLAY):\n",
        "\n",
        "    states_init = np.zeros((1,500, 10)) + np.random.randint(20, 34, size=(10))\n",
        "    states_init = torch.FloatTensor(states_init)\n",
        "    logits = policy(states_init) # forward pass\n",
        "    probs = nn.functional.softmax(logits, -1) # get estimated actions for states\n",
        "    #log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    actions = []\n",
        "    actions_probs_policy = np.squeeze(probs).detach().numpy()     \n",
        "    for prob in actions_probs_policy:                  \n",
        "        actions.append(np.random.choice(4,  p = prob))\n",
        "\n",
        "    # Apply dict to go from 1,2,3,4 as action to [0,2400,0] [0,2400,1000]  [2300,0,0]  [2300,0,1000] \n",
        "    my_dict = {0:[0,2400,0], 1:[0,2400,1000], 2:[2300,0,0], 3:[2300,0,1000]}\n",
        "    actions_array = np.zeros((500,3))  \n",
        "    actions_array = map(my_dict.get, actions)\n",
        "\n",
        "    actions_array = np.array(list(actions_array))\n",
        "    actions_array_sysmodel = np.concatenate([np.arange(actions_array.shape[0])[:,None]+1,actions_array], axis=1) \n",
        "    actions_array_sysmodel = tf.expand_dims(actions_array_sysmodel, axis = 0, name=None)\n",
        "\n",
        "    states = sysmodel(actions_array_sysmodel)\n",
        "\n",
        "    states = states*norm_params[:,0]+norm_params[:,1]\n",
        "\n",
        "    # Recursively get expected discounted rewards (rewards given by the current cost function)\n",
        "    rewards = get_rewards(states, actions)\n",
        "    cumulative_returns_np = np.array(get_expected_rewards(rewards, 0.9))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns_np, dtype=torch.float32)\n",
        "\n",
        "    actions_tensor = torch.from_numpy(np.array(actions))\n",
        "\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        probs * to_one_hot(actions_tensor, 4), dim=1)\n",
        "\n",
        "    #entropy = -torch.mean(torch.sum(probs_samp*log_probs_samp), dim = -1 )\n",
        "    #loss = -torch.mean(log_probs_for_actions*cumulative_returns - entropy*1e-2) # loss for the policy (isnt it the cost function output?)\n",
        "    #average reward baseline\n",
        "    cumulative_returns = (cumulative_returns - torch.mean(cumulative_returns))\n",
        "    #print(cumulative_returns)\n",
        "    #whitening baseline\n",
        "    #cumulative_returns = (cumulative_returns - torch.mean(cumulative_returns))/ (torch.std(cumulative_returns))\n",
        "    loss_policy = -log_probs_for_actions*cumulative_returns\n",
        "    # UPDATING THE POLICY NETWORK\n",
        "    #policy_optimizer.zero_grad()\n",
        "    print(loss_policy.sum().detach().numpy())\n",
        "    loss_policy.sum().backward()\n",
        "    torch.nn.utils.clip_grad_norm_(policy.parameters(), 0.01)\n",
        "    optimizer.step()\n",
        "\n",
        "    #returns = sum(rewards)\n",
        "    sum_of_loss = loss_policy.sum().detach().numpy()\n",
        "    #return_list.append(returns)\n",
        "    sum_of_loss_list.append(sum_of_loss)\n",
        "    states_plot = np.squeeze(states)\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.title(f\"Mean loss per {EPISODES_TO_PLAY} episodes\")\n",
        "    plt.plot(mean_loss)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.title(f\"Rewards - last series\")\n",
        "    plt.plot(rewards)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.title(f\"Preheat phase Activations - last series\")\n",
        "    plt.plot(actions[450:500])\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.title(f\"Preheat phase Temperatures - last series\")\n",
        "    plt.plot(states_plot[0:50,:])\n",
        "    plt.grid()\n",
        "\n",
        "    # plt.show()\n",
        "    plt.savefig('GCL_learning_curve.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl1bFS0vfpqo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.6 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2276c0bad7efeb8779f7fdd76186059b8fbce5d8b401674f0514107ca347933d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
