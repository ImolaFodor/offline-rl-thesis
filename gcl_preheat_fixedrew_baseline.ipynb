{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import myModelinTF\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from torch.autograd import Variable \n",
        "\n",
        "from statistics import mean "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train firsts set size: 495\n"
          ]
        }
      ],
      "source": [
        "demo = np.load('dataset_train_firsts.npy', allow_pickle=True)\n",
        "print(\"Train firsts set size: \" + str(len(demo)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SeriesDataset(Dataset):\n",
        "\n",
        "    def __init__(self, file):\n",
        "        self.demo = np.load(file, allow_pickle=True)\n",
        "\n",
        "    #len(usd)\n",
        "    def __len__(self):\n",
        "        return len(self.demo)\n",
        "\n",
        "    # a_list[1] --> a__list.__getitem__(1)\n",
        "    def __getitem__(self, index):\n",
        "        run =self.demo[index]\n",
        "        states = np.array(run[0]).T\n",
        "        #states_init1 = states[1,:]\n",
        "        if states.size == 0:\n",
        "            print('no states in submatrix')\n",
        "\n",
        "        actions = np.array(run[1])[:, None]\n",
        "\n",
        "        action_count = np.array([actions[0][0], len(actions)])\n",
        "        #actions_counts_array_padded = np.pad(actions_counts_array, ((0,30-len(actions_counts_array)),(0,0)), 'constant').astype('int32') \n",
        "        #states_init = np.zeros((len(actions_counts_array_padded), 10)) + states_init1\n",
        "        #states_init = states[0:len(actions_counts_array),:]\n",
        "        print(states[:,1])\n",
        "        return states[:,1], action_count.astype('int32') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "def permute_them(x):\n",
        "  return torch.permute(x, (1, 0, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fill_four_with_last(arr):\n",
        "    prev = np.arange(len(arr))\n",
        "    prev[arr == 4] = 0\n",
        "    prev = np.maximum.accumulate(prev)\n",
        "\n",
        "    # it can be that the first element is 4, so remove those\n",
        "    temp = arr[prev]\n",
        "    new_deleted = np.delete(temp, np.where(temp == 4))\n",
        "    count = len(temp) - len(new_deleted)\n",
        "    new = np.insert(new_deleted, 0, np.repeat(new_deleted[0], count), axis=0)\n",
        "    return new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PG(nn.Module):\n",
        "    def __init__(self, n_actions):\n",
        "        super().__init__()\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        # Policy takes input initial conditions and outputs planning route, open-loop control\n",
        "        self.actor_model = nn.Sequential(\n",
        "            nn.LSTM(input_size = 1, hidden_size = self.n_actions, num_layers = 2, dropout = 0.1,batch_first = True),\n",
        "        )\n",
        "\n",
        "        self.logprobs = []\n",
        "        self.state_values = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, states):\n",
        "        logits, (h_T, c_T)  = self.actor_model(states)\n",
        "        return logits[-1]\n",
        "    # Run agent in environment to create sample trajectories by generator\n",
        "    # The environment model is a seq2seq model\n",
        "    def generate_session(self, sysmodel, init, traj_idx,  t_max=200):\n",
        "        norm_params = pd.read_csv('D:\\\\units\\\\thesis\\\\thesis\\\\train_reward_inverse_rl_w_sys_model\\\\norm_params.csv', header = None).to_numpy().T\n",
        "\n",
        "        states, actions, actions_probs_policy, rewards = [], [], [], []\n",
        "        init_idx = init[traj_idx]\n",
        "        states_init_idx = init_idx[0]\n",
        "        init_COT_idx = states_init_idx[1,:]\n",
        "\n",
        "        states = torch.FloatTensor(init_COT_idx.T)\n",
        "\n",
        "        states = states.unsqueeze(0)\n",
        "        states_history = states[-1,:].unsqueeze(0)\n",
        "        list_actions_taken = []\n",
        "        actions_probs_policy_taken = []\n",
        "        action = 0\n",
        "        for t in range(0, t_max, 5):\n",
        "            logits = policy(states) # forward pass\n",
        "            \n",
        "            probs = nn.functional.softmax(logits, -1) # get estimated actions for states\n",
        "\n",
        "            actions_probs_policy = np.squeeze(probs).clone().detach().numpy()\n",
        "            actions_probs_policy_rep = np.tile(actions_probs_policy,(5,1))\n",
        "            actions_probs_policy_taken.append(actions_probs_policy_rep)\n",
        "\n",
        "            action_prev = action\n",
        "            action = np.random.choice(5,  p = actions_probs_policy)\n",
        "            if action == 4:\n",
        "                action = action_prev\n",
        "\n",
        "            action_rep = np.repeat(action, 5)\n",
        "            list_actions_taken.append(action_rep)\n",
        "\n",
        "            #list_actions_filled = np.append(list_actions_filled, a)\n",
        "            \n",
        "            # Apply dict to go from 1,2,3,4 as action to [0,2400,0] [0,2400,1000]  [2300,0,0]  [2300,0,1000] \n",
        "            my_dict = {0:[0,2400,0], 1:[0,2400,1000], 2:[2300,0,0], 3:[2300,0,1000]}\n",
        "            #actions_array = np.zeros((500,3))  \n",
        "            actions_array = map(my_dict.get, np.array(list_actions_taken).flatten())\n",
        "            actions_array = np.array(list(actions_array))\n",
        "\n",
        "            actions_array_sysmodel = np.concatenate([np.arange(actions_array.shape[0])[:,None]+1+t*len(list_actions_taken),actions_array], axis=1) \n",
        "            actions_array_sysmodel = tf.expand_dims(actions_array_sysmodel, axis = 0, name=None)\n",
        "            \n",
        "            states = sysmodel(actions_array_sysmodel)\n",
        "\n",
        "            # Revert normalized values for states\n",
        "            states = states*norm_params[:,0]+norm_params[:,1]\n",
        "            states = torch.FloatTensor(np.array(states[:,:,1]))\n",
        "            # More actions fed to the sysmodel, higher the accuracy, but the resulting states will not be the ones effectively experienced, so history is needed\n",
        "            # Rewards can be calculated on the set of states experienced with all actions at once, but then the scope is just to evaluate the set of actions in the whole episode, not step by step\n",
        "            states_history = torch.cat((states_history, states[-5:]), 1)\n",
        "\n",
        "            # if done:\n",
        "            #     break\n",
        "        \n",
        "        # print(actions_taken)\n",
        "        # actions_taken = np.array(actions_taken)\n",
        "        actions_probs_policy_taken = np.array(actions_probs_policy_taken).reshape(t_max, 5)\n",
        "        # states_all = sysmodel(actions_taken_reshape)\n",
        "        # states_all = states_new*norm_params[:,0]+norm_params[:,1]\n",
        "        rewards = get_rewards(states, np.array(list_actions_taken), t_max)\n",
        "        return states_history, states, np.array(list_actions_taken).flatten(), actions_probs_policy_taken, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ValueFunc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.value_model = nn.Sequential(\n",
        "            nn.Linear(5, 32),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = torch.FloatTensor(state) #.unsqueeze(0)\n",
        "        state_value = self.value_model(state)\n",
        "        return state_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ind(array, t_max):\n",
        "    idx_return = (t_max + 1,0)\n",
        "    for idx, val in np.ndenumerate(array):\n",
        "        if val <= 201 and val >= 199:\n",
        "            idx_return = idx\n",
        "            break\n",
        "    return int(idx_return[0])\n",
        "    # If no item was found return None, other return types might be a problem due to\n",
        "    # numbas type inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "_DYTtGk7Fa1h"
      },
      "outputs": [],
      "source": [
        "def get_y_rewards(model,states, rewards_loc,gamma=0.9):\n",
        "    states = states.numpy()\n",
        "    for idx_i in range(len(rewards_loc)):\n",
        "        for idx in range(idx_i + 1, len(rewards_loc)):\n",
        "            input1 = states[:,idx]\n",
        "            for i in range(4):\n",
        "                input1 = np.concatenate((input1,[np.mean(states[:,idx-5*(i+1):])]))\n",
        "            out1 = model(input1).squeeze()\n",
        "            rewards_loc[idx_i] += out1.detach().numpy()\n",
        "            continue\n",
        "    return rewards_loc\n",
        "\n",
        "def get_expected_rewards(model,states, rewards,gamma=0.9):\n",
        "    G = np.zeros_like(rewards, dtype=float)\n",
        "    states = states.numpy()\n",
        "    for idx in range(-1, -len(rewards), -1):\n",
        "        input1 = states[:,idx]\n",
        "        for i in range(4):\n",
        "            input1 = np.concatenate((input1, [np.mean(states[:,idx-5*(i+1):])]))\n",
        "        G[idx] = model(input1).detach().numpy() + G[idx+1]\n",
        "    return G\n",
        "\n",
        "# For REINFORCE with baseline\n",
        "def get_rewards_to_go(rewards,gamma=0.9):\n",
        "    G = np.zeros_like(rewards, dtype=float)\n",
        "    for idx in range(-1, -len(rewards), -1):\n",
        "        G[idx] = rewards[idx] + gamma * G[idx+1]\n",
        "\n",
        "    return G\n",
        "\n",
        "def get_advantage_rewards(model,states, rewards,gamma=0.9):\n",
        "    G = np.zeros_like(rewards, dtype=float)\n",
        "    #G[-1] = rewards[-1]\n",
        "    states = states.numpy()\n",
        "    for idx in range(-1, -len(rewards)-1, -1):\n",
        "        input1 = states[:,idx]\n",
        "        for i in range(4):\n",
        "            input1 = np.concatenate((input1,[np.mean(states[:,idx-5*(i+1):])]))\n",
        "\n",
        "        input2 = states[:,idx + 1]\n",
        "        for i in range(4):\n",
        "            input2 = np.concatenate((input2, [np.mean(states[:,(idx + 1)-5*(i+1):])]))\n",
        "\n",
        "        out1 = model(input1) + out1\n",
        "        out2 = model(input2)\n",
        "        G[idx] = rewards[idx] + out2.detach().numpy() - out1.detach().numpy()\n",
        "    return G\n",
        "\n",
        "def get_rewards(states, actions, t_max):\n",
        "    \n",
        "    states = states.numpy()\n",
        "    states = states.squeeze()\n",
        "    rewards = np.zeros((np.shape(states)[0],1))\n",
        "\n",
        "    actions = actions.flatten()\n",
        "    # print(actions)\n",
        "    max_hotspot =  np.max(states)\n",
        "    if max_hotspot < 280:\n",
        "        rewards[-1] = 100\n",
        "\n",
        "    target_first_reached_at = ind(states,t_max)\n",
        "\n",
        "    print(f\"Reached Target 230C at step {target_first_reached_at} with t_max = {t_max}\")\n",
        "\n",
        "    if target_first_reached_at < t_max and target_first_reached_at > (t_max - 180):\n",
        "        rewards[-1] = rewards[-1] + 500\n",
        "\n",
        "        \n",
        "    for idx in range(np.shape(states)[0]):\n",
        "        #idx = idx -1\n",
        "        # if temps.any() > 230:\n",
        "        #     rewards[idx] = rewards[idx] -30\n",
        "        \n",
        "        # subset = actions[idx-20:]\n",
        "        # if np.sum(np.diff(subset)) == 0:\n",
        "        #     rewards[idx] = rewards[idx] + 20\n",
        "\n",
        "        # subset = states[idx-10:]\n",
        "        # if np.absolute(np.sum(np.diff(subset))) < 10:\n",
        "        #     #print('Difference between states not big')\n",
        "        #     rewards[idx] = rewards[idx] + 70\n",
        "\n",
        "        # Save power consumption\n",
        "        # if actions[idx] == 1 | actions[idx] == 3:\n",
        "        #     rewards[idx] = rewards[idx] -10\n",
        "\n",
        "        # Action 4 is encouraged , has the meaning to keep the previous action\n",
        "        if actions[idx] == 4:\n",
        "            #print('Reward for keeping previous action')\n",
        "            rewards[idx] = rewards[idx] + 100\n",
        "\n",
        "        # Negative temp goes with high negative reward\n",
        "        if states[idx] < 0:\n",
        "            #print('Penalizing for negative values')\n",
        "            rewards[idx] = rewards[idx] -200\n",
        "\n",
        "        # Penalize if previous state higher than current (variation)\n",
        "        temps_subset = np.array([states[idx], states[idx-1]])\n",
        "        if np.diff(temps_subset) <= 0:\n",
        "            #print('Penalizing for decreasing or steady state value')\n",
        "            rewards[idx] = rewards[idx] - 500\n",
        "\n",
        "        if 0 > np.diff(temps_subset) <= 10 :\n",
        "            #print('Penalizing for decreasing or steady state value')\n",
        "            rewards[idx] = rewards[idx] + 200\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def to_one_hot(y_tensor, ndims):\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_demos(demos, traj_idx):\n",
        "    demo = demos[traj_idx]\n",
        "    states_demo = demo[0]\n",
        "    \n",
        "    COT_demo = states_demo[1]\n",
        "    states = torch.FloatTensor(COT_demo.T)\n",
        "    states = states.unsqueeze(0)\n",
        "\n",
        "    list_actions_taken = np.array(demo[1])\n",
        "    actions_probs_policy_taken = demo[2]\n",
        "\n",
        "    rewards = get_rewards(states, list_actions_taken, 500)\n",
        "    \n",
        "    return states, list_actions_taken, actions_probs_policy_taken, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IM1Ki1hL2U42",
        "outputId": "975040a9-996f-4879-cf39-9c3f89ebbc25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reached Target 230C at step 501 with t_max = 500\n",
            "Reached Target 230C at step 498 with t_max = 500\n",
            "Reached Target 230C at step 501 with t_max = 500\n"
          ]
        }
      ],
      "source": [
        "# DEVICE\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "# ENV SETUP\n",
        "sysmodel = myModelinTF.load_model()\n",
        "n_actions = 5\n",
        "\n",
        "# INITILIZING POLICY AND REWARD FUNCTION\n",
        "policy = PG(n_actions)\n",
        "policy.to(DEVICE)\n",
        "\n",
        "value = ValueFunc()\n",
        "value.to(DEVICE)\n",
        "\n",
        "norm_params = pd.read_csv('D:\\\\units\\\\thesis\\\\thesis\\\\train_reward_inverse_rl_w_sys_model\\\\norm_params.csv', header = None).to_numpy().T\n",
        "\n",
        "\n",
        "# LOADING EXPERT/DEMO SAMPLES\n",
        "DATASETNPY = \"dataset_train_firsts.npy\"\n",
        "\n",
        "DATASET_DEMO = \"dataset_train.npy\"\n",
        "\n",
        "#dataset_inits = SeriesDataset(DATASETNPY)\n",
        "\n",
        "##### TUNE MODEL WITH REWARD LOSS\n",
        "\n",
        "mean_rewards = []\n",
        "mean_loss = []\n",
        "mean_loss_rew = []\n",
        "EPISODES_TO_PLAY = 20\n",
        "EPISODES_FROM_DEMO = 50\n",
        "sample_trajs = []\n",
        "\n",
        "return_list, sum_of_loss_list, loss_value_list = [], [], []\n",
        "\n",
        "# Retrain all params\n",
        "optimizer_policy = torch.optim.Adam(policy.parameters(), 1e-1, weight_decay=1e-2)\n",
        "optimizer_value = torch.optim.Adam(value.parameters(), 1e-1, weight_decay=1e-2)\n",
        "\n",
        "init_states = np.load(DATASETNPY, allow_pickle=True)\n",
        "demos = np.load(DATASET_DEMO, allow_pickle=True)\n",
        "\n",
        "criterion2 = nn.MSELoss()\n",
        "\n",
        "t_max = 500\n",
        "\n",
        "#Epochs\n",
        "for i in range(5):\n",
        "    trajs_policy = [policy.generate_session(sysmodel, init_states, l, t_max) for l in range(EPISODES_TO_PLAY)]\n",
        "    trajs_demo = [sample_demos(demos, l) for l in range(EPISODES_FROM_DEMO)] \n",
        "    # Episodes\n",
        "    for traj in trajs_demo:\n",
        "        states, actions, probs, rewards = traj\n",
        "        \n",
        "        rewards_plot = np.array(rewards.squeeze())\n",
        "        expected_returns_np = np.array(get_expected_rewards(value,states,rewards, 0.7)).squeeze()\n",
        "        expected_returns = torch.tensor(expected_returns_np, dtype=torch.float32)\n",
        "\n",
        "        R_np = np.array(get_rewards_to_go(rewards, 0.7)).squeeze()\n",
        "        R = torch.tensor(R_np, dtype=torch.float32)\n",
        "\n",
        "        cumulative_returns = (R - expected_returns) # torch.mean\n",
        "\n",
        "        actions_tensor = torch.from_numpy(actions)\n",
        "        probs_tensor = torch.from_numpy(np.array(probs))\n",
        "\n",
        "        log_probs_for_actions = -torch.sum(to_one_hot(actions_tensor, 4), dim =1)\n",
        "\n",
        "        loss_policy = -log_probs_for_actions*cumulative_returns\n",
        "        # UPDATING THE POLICY NETWORK\n",
        "        optimizer_policy.zero_grad()\n",
        "        loss_policy.requires_grad = True\n",
        "        loss_policy.sum().backward()\n",
        "        torch.nn.utils.clip_grad_norm_(policy.parameters(), 0.01)\n",
        "        optimizer_policy.step()\n",
        "\n",
        "        loss_value = criterion2(torch.from_numpy(np.array(get_expected_rewards(value,states,rewards, 0.7).sum())).to(torch.float32), \\\n",
        "                                 torch.from_numpy(np.array(get_rewards_to_go(rewards, 0.7).sum())).to(torch.float32))\n",
        "        # UPDATING THE VALUE NETWORK\n",
        "        optimizer_value.zero_grad()\n",
        "        loss_value.requires_grad = True\n",
        "        loss_value.backward()\n",
        "        optimizer_value.step()\n",
        "\n",
        "        #returns = sum(rewards)\n",
        "        sum_of_loss = loss_policy.sum().detach().numpy()\n",
        "        print(f\"\\nLoss Actor: {sum_of_loss}\\n\")\n",
        "        print(f\"\\nLoss Value: {loss_value}\\n\")\n",
        "        #print(f\"\\nCumulative return: {cumulative_returns}\\n\")\n",
        "        #return_list.append(returns)\n",
        "        sum_of_loss_list.append(sum_of_loss)\n",
        "        states_plot = np.array(states.squeeze())\n",
        "\n",
        "        plt.subplot(2, 3, 1)\n",
        "        plt.title(f\"Policy Loss\")\n",
        "        plt.plot(sum_of_loss_list)\n",
        "        plt.grid()\n",
        "\n",
        "        plt.subplot(2, 3, 2)\n",
        "        plt.title(f\"Value Loss\")\n",
        "        plt.plot(loss_value_list)\n",
        "        plt.grid()\n",
        "\n",
        "        plt.subplot(2, 3, 3)\n",
        "        plt.title(f\"Rewards\")\n",
        "        plt.plot(rewards_plot)\n",
        "        plt.grid()\n",
        "\n",
        "        plt.subplot(2, 3, 4)\n",
        "        plt.title(f\"Activations\")\n",
        "        plt.plot(actions[0:500])\n",
        "        plt.grid()\n",
        "\n",
        "        plt.subplot(2, 3, 6)\n",
        "        plt.title(f\"Temps Planned\")\n",
        "        plt.plot(states_plot[0:500], color = 'm')\n",
        "        plt.grid()\n",
        "\n",
        "        # plt.show()\n",
        "        plt.savefig('GCL_learning_curve.png')\n",
        "        plt.close()\n",
        "\n",
        "    for traj in trajs_policy:\n",
        "        states_history, states, actions, probs, rewards = traj\n",
        "        \n",
        "        rewards_plot = np.array(rewards.squeeze())\n",
        "        expected_returns_np = np.array(get_expected_rewards(value,states,rewards, 0.7)).squeeze()\n",
        "        expected_returns = torch.tensor(expected_returns_np, dtype=torch.float32)\n",
        "\n",
        "        R_np = np.array(get_rewards_to_go(rewards, 0.7)).squeeze()\n",
        "        R = torch.tensor(R_np, dtype=torch.float32)\n",
        "\n",
        "        cumulative_returns = (R - expected_returns) # torch.mean\n",
        "\n",
        "        actions_tensor = torch.from_numpy(actions)\n",
        "        probs_tensor = torch.from_numpy(np.array(probs))\n",
        "\n",
        "        log_probs_for_actions = torch.log(torch.sum(\n",
        "            probs_tensor * to_one_hot(actions_tensor, 5), dim =1))\n",
        "\n",
        "        loss_policy = -log_probs_for_actions*cumulative_returns\n",
        "        # UPDATING THE POLICY NETWORK\n",
        "        optimizer_policy.zero_grad()\n",
        "        loss_policy.requires_grad = True\n",
        "        loss_policy.sum().backward()\n",
        "        torch.nn.utils.clip_grad_norm_(policy.parameters(), 0.01)\n",
        "        optimizer_policy.step()\n",
        "\n",
        "        loss_value = criterion2(torch.from_numpy(np.array(get_expected_rewards(value,states,rewards, 0.7).sum())).to(torch.float32), \\\n",
        "                                 torch.from_numpy(np.array(get_rewards_to_go(rewards, 0.7).sum())).to(torch.float32))\n",
        "        # UPDATING THE VALUE NETWORK\n",
        "        optimizer_value.zero_grad()\n",
        "        loss_value.requires_grad = True\n",
        "        loss_value.backward()\n",
        "        optimizer_value.step()\n",
        "\n",
        "        #returns = sum(rewards)\n",
        "        sum_of_loss = loss_policy.sum().detach().numpy()\n",
        "        print(f\"\\nLoss Actor: {sum_of_loss}\\n\")\n",
        "        print(f\"\\nLoss Value: {loss_value}\\n\")\n",
        "        #print(f\"\\nCumulative return: {cumulative_returns}\\n\")\n",
        "        #return_list.append(returns)\n",
        "        sum_of_loss_list.append(sum_of_loss)\n",
        "        states_history_plot = np.array(states_history.squeeze())\n",
        "        states_plot = np.array(states.squeeze())\n",
        "        loss_value_list.append(loss_value)\n",
        "\n",
        "        plt.subplot(2, 3, 1)\n",
        "        plt.title(f\"Policy Loss\")\n",
        "        plt.plot(sum_of_loss_list)\n",
        "        plt.grid()\n",
        "\n",
        "        plt.subplot(2, 3, 2)\n",
        "        plt.title(f\"Value Loss\")\n",
        "        plt.plot(loss_value_list)\n",
        "        plt.grid()\n",
        "\n",
        "        plt.subplot(2, 3, 3)\n",
        "        plt.title(f\"Rewards\")\n",
        "        plt.plot(rewards_plot)\n",
        "        plt.grid()\n",
        "\n",
        "        plt.subplot(2, 3, 4)\n",
        "        plt.title(f\"Activations\")\n",
        "        plt.plot(actions[0:500])\n",
        "        plt.grid()\n",
        "\n",
        "        plt.subplot(2, 3, 5)\n",
        "        plt.title(f\"Temps Actual\")\n",
        "        plt.plot(states_history_plot[0:500], color='red')\n",
        "        plt.grid()\n",
        "\n",
        "        plt.subplot(2, 3, 6)\n",
        "        plt.title(f\"Temps Planned\")\n",
        "        plt.plot(states_plot[0:500], color = 'm')\n",
        "        plt.grid()\n",
        "\n",
        "        # plt.show()\n",
        "        plt.savefig('GCL_learning_curve.png')\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.6 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2276c0bad7efeb8779f7fdd76186059b8fbce5d8b401674f0514107ca347933d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
