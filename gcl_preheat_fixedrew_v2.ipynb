{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import myModelinTF\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from torch.autograd import Variable "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cumcount(arr):\n",
        "    result = []\n",
        "    count = 0\n",
        "    curr_individual = arr[0]\n",
        "    result.append(count)\n",
        "    for value in arr[1:]:        \n",
        "        if value == curr_individual:\n",
        "            count += 1\n",
        "            result.append(count)         \n",
        "        else :\n",
        "            count = 0\n",
        "            result.append(count)\n",
        "            curr_individual = value\n",
        "    return result\n",
        "\n",
        "# input_array = np.array([1, 1, 1, 2, 2, 1, 1, 1, 1])\n",
        "# output_array = cumcount(input_array)\n",
        "# print(output_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A = np.array([[1,2],[3,4],[1,2],[3,4]])\n",
        "\n",
        "# print(np.pad(A, ((0,5-len(A)),(0,0)), 'constant'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SeriesDataset(Dataset):\n",
        "\n",
        "    def __init__(self, file):\n",
        "        self.demo = np.load(file, allow_pickle=True)\n",
        "\n",
        "    #len(usd)\n",
        "    def __len__(self):\n",
        "        return len(self.demo)\n",
        "\n",
        "    # a_list[1] --> a__list.__getitem__(1)\n",
        "    def __getitem__(self, index):\n",
        "        run =self.demo[index]\n",
        "\n",
        "        states = np.array(run[0]).T\n",
        "        states_init1 = states[1,:]\n",
        "        actions = np.array(run[1])[:, None]\n",
        "\n",
        "        df_actions_counts = pd.DataFrame(cumcount(np.squeeze(actions)),columns =['cum_counts'])\n",
        "        # the last value is padded with Nan, so putting 1000 as value, to not fetch with the 0\n",
        "        df_actions_counts['lead_counts'] = df_actions_counts['cum_counts'].shift(-2).replace(np.nan, 1000)\n",
        "        df_actions_counts['actions'] = np.squeeze(actions).tolist()\n",
        "        df_actions_counts_at_0 = df_actions_counts.loc[df_actions_counts['lead_counts'] == 0]\n",
        "        df_actions_counts['cum_counts'] = df_actions_counts['cum_counts']+1\n",
        "        actions_counts_array = df_actions_counts_at_0[['actions', 'cum_counts']]\n",
        "        actions_counts_array_padded = np.pad(actions_counts_array, ((0,500-len(actions_counts_array)),(0,0)), 'constant').astype('int32') \n",
        "\n",
        "        states_init = np.zeros((len(actions_counts_array_padded), 10)) + states_init1\n",
        "        return states_init, actions_counts_array_padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def permute_them(x):\n",
        "  return torch.permute(x, (1, 0, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "\n",
        "    tensors, targets = [], []\n",
        "\n",
        "    for states_init, actions in batch:\n",
        "        tensors += [torch.tensor(states_init).type(torch.FloatTensor)]\n",
        "        targets += [torch.tensor(actions).type(torch.LongTensor)]\n",
        "\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    return tensors, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PG(nn.Module):\n",
        "    def __init__(self, state_shape, n_actions):\n",
        "        super().__init__()\n",
        "        self.state_shape = state_shape\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        # Policy takes input initial conditions and outputs planning route, open-loop control\n",
        "        self.model = nn.Sequential(\n",
        "            nn.LSTM(input_size = 10, hidden_size = 5, num_layers = 2, dropout = 0.1,batch_first = True),\n",
        "            \n",
        "        )\n",
        "\n",
        "    def forward(self, states):\n",
        "        #states = torch.FloatTensor(states)\n",
        "        logits, (h_T, c_T)  = self.model(states)\n",
        "        return logits\n",
        "    \n",
        "    def predict_probs(self, states):\n",
        "        states = torch.FloatTensor(states)\n",
        "        logits, (h_T, c_T) = self.model(states)\n",
        "        probs = F.softmax(logits, dim = -1).detach().numpy()\n",
        "        return probs\n",
        "    \n",
        "    # Run agent in environment to create sample trajectories by generator\n",
        "    # The environment model is a seq2seq model\n",
        "    def generate_session(self, sysmodel, t_max=1000):\n",
        "        norm_params = pd.read_csv('D:\\\\units\\\\thesis\\\\thesis\\\\train_reward_inverse_rl_w_sys_model\\\\norm_params.csv', header = None).to_numpy().T\n",
        "\n",
        "        states, actions, traj_probs, rewards = [], [], [], []\n",
        "        states_init = np.zeros((500,1, 10)) + np.random.randint(20, 34, size=(10)) \n",
        "        actions_probs_policy = self.predict_probs(states_init)\n",
        "        actions = []\n",
        "        actions_probs_policy = np.squeeze(actions_probs_policy)     \n",
        "        for prob in actions_probs_policy:                  \n",
        "            actions.append(np.random.choice(self.n_actions,  p = prob))\n",
        " \n",
        "        # Apply dict to go from 1,2,3,4 as action to [0,2400,0] [0,2400,1000]  [2300,0,0]  [2300,0,1000] \n",
        "        my_dict = {0:[0,2400,0], 1:[0,2400,1000], 2:[2300,0,0], 3:[2300,0,1000]}\n",
        "        actions_array = np.zeros((500,3))  \n",
        "        actions_array = map(my_dict.get, actions)\n",
        "\n",
        "        actions_array = np.array(list(actions_array))\n",
        "        actions_array_sysmodel = np.concatenate([np.arange(actions_array.shape[0])[:,None]+1,actions_array], axis=1) \n",
        "        actions_array_sysmodel = tf.expand_dims(actions_array_sysmodel, axis = 0, name=None)\n",
        "\n",
        "        states = sysmodel(actions_array_sysmodel)\n",
        "        states = states*norm_params[:,0]+norm_params[:,1]\n",
        "        \n",
        "        return states, actions, actions_probs_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, tepoch):\n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "    batch_idx = 0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        correct = 0\n",
        "        data = torch.stack(data)\n",
        "        \n",
        "        output = model(data)\n",
        "        target0 = to_one_hot(target[:,:,0], 4).float()\n",
        "        \n",
        "        output0 = torch.flatten(output[:,:,0:4], start_dim = 0, end_dim = 1)\n",
        "        loss = criterion2(torch.squeeze(output[:,:,4]), torch.squeeze(target[:,:,1]).float()) + criterion1(output0, target0) \n",
        "\n",
        "        output_np = output.detach().numpy().tolist()\n",
        "        target_np = target.detach().numpy().tolist()  \n",
        "        for (output, target) in zip(output_np, target_np):        \n",
        "            if output.index(max(output)) == target.index(max(target)):\n",
        "                correct +=1\n",
        "        \n",
        "        accuracy = correct / (500 * BATCH_SIZE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print training stats\n",
        "        print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\\tAccuracy of batch: {accuracy:.6f}\")\n",
        "        # record loss\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "\n",
        "        batch_idx += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(model, epoch):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "\n",
        "    for data, target in test_loader:\n",
        "        data = torch.stack(data)\n",
        "\n",
        "        output = model(data)\n",
        "        target0 = to_one_hot(target[:,:,0], 4)\n",
        "        \n",
        "        # action to take\n",
        "        output0 = torch.flatten(output[:,:,0:4], start_dim = 0, end_dim = 1)\n",
        "        # aount\n",
        "        output1 = torch.squeeze(output[:,:,4])\n",
        "\n",
        "        output_np = output0.detach().numpy().tolist()\n",
        "        target_np = target0.detach().numpy().tolist()\n",
        "        correct = 0\n",
        "        for (output3, target) in zip(output_np, target_np):           \n",
        "            if output3.index(max(output3)) == target.index(max(target)):\n",
        "                correct +=1\n",
        "\n",
        "        accuracy = correct / (500 * BATCH_SIZE)\n",
        "\n",
        "    print(f\"\\nTest Epoch: {epoch}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\tAccuracy: {accuracy}\\n\")\n",
        "    for out in output:\n",
        "        probs = nn.functional.softmax(out[:,0:4], -1) # get estimated actions for states\n",
        "        actions = []\n",
        "        A = [0,1,2,3]\n",
        "        probs = np.squeeze(probs).detach().numpy()\n",
        "        \n",
        "        for prob in probs:                  \n",
        "            actions.append(A[np.argmax(prob)])\n",
        "\n",
        "        print(actions)\n",
        "        print(out[:,4])\n",
        "\n",
        "        \n",
        "        plt.subplot(2, 1, 1)\n",
        "        plt.title(f\"Example Initial state\")\n",
        "        plt.plot(np.squeeze(data).detach().numpy())\n",
        "        plt.grid()\n",
        "\n",
        "        # plt.subplot(2, 1, 2)\n",
        "        # plt.title(f\"Actions\")\n",
        "        # plt.plot(actions)\n",
        "        plt.subplot(2,1,2)\n",
        "\n",
        "        plt.stairs(actions, linewidth=2.5)\n",
        "\n",
        "        plt.xlim=(0, 8)\n",
        "        plt.xticks=out[:,4]\n",
        "        plt.ylim=(0, 3)\n",
        "        plt.yticks=[0,1,2,3]\n",
        "        plt.grid()\n",
        "\n",
        "        # plt.show()\n",
        "        plt.savefig('Train LSTM_action sequence.png')\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_DYTtGk7Fa1h"
      },
      "outputs": [],
      "source": [
        "def number_of_correct(pred, target):\n",
        "    # count number of correct predictions\n",
        "    return pred.squeeze().eq(target).sum().item()\n",
        "\n",
        "def get_likely_index(tensor):\n",
        "    # find most likely label index for each element in the batch\n",
        "    return tensor.argmax(dim=-1)\n",
        "\n",
        "def compute_error(x, y):\n",
        "    Ps = torch.norm(x)\n",
        "    Pn = torch.norm(x-y)\n",
        "    return 20*torch.log10(Ps/Pn)\n",
        "\n",
        "def get_expected_rewards(rewards,gamma=0.9):\n",
        "    G = np.zeros_like(rewards, dtype=float)\n",
        "    G[-1] = rewards[-1]\n",
        "    for idx in range(-2, -len(rewards)-1, -1):\n",
        "        G[idx] = rewards[idx] + gamma * G[idx+1]\n",
        "    return G\n",
        "\n",
        "def get_rewards(states, actions):\n",
        "    rewards = np.zeros((500,1))\n",
        "\n",
        "    states = states.numpy()\n",
        "    states = states.squeeze()\n",
        "\n",
        "    max_hotspot =  np.max(states)\n",
        "    if max_hotspot < 300:\n",
        "        rewards[-1] = 50\n",
        "\n",
        "    if np.where(states[:,1] == 230)[0] < 405 & np.where(states[:,1] == 230)[0] > 340:\n",
        "        rewards[-1] = rewards[-1] + 40\n",
        "        \n",
        "    for idx in range(np.shape(states)[0]):\n",
        "        temps = np.array(states[idx])\n",
        "        if temps.all() > 300:\n",
        "            rewards[idx] = rewards[idx] -30\n",
        "        \n",
        "        subset = actions[idx-30:]\n",
        "        if np.sum(np.diff(subset)) == 0:\n",
        "            rewards[idx] = rewards[idx] + 20\n",
        "\n",
        "        subset = temps[idx-20:]\n",
        "        if np.sum(np.diff(subset)) < 10:\n",
        "            rewards[idx] = rewards[idx] + 20\n",
        "\n",
        "        if actions[idx] == 1 | actions[idx] == 3:\n",
        "            rewards[idx] = rewards[idx] -10\n",
        "\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def to_one_hot(y_tensor, ndims):\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONVERTS TRAJ LIST TO STEP LIST\n",
        "def preprocess_traj(traj_list, step_list):\n",
        "    step_list = step_list.tolist()\n",
        "    for traj in traj_list:\n",
        "        states = np.array(traj[0])\n",
        "        probs = np.ones((500,4))\n",
        "        states = np.transpose(traj[0])\n",
        "        actions = np.array(traj[1])[:, None]\n",
        "\n",
        "        x = np.concatenate((states, actions, probs), axis=1) \n",
        "        step_list.extend(x)\n",
        "    return np.array(step_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IM1Ki1hL2U42",
        "outputId": "975040a9-996f-4879-cf39-9c3f89ebbc25"
      },
      "outputs": [],
      "source": [
        "# DEVICE\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "# ENV SETUP\n",
        "sysmodel = myModelinTF.load_model()\n",
        "n_actions = 4\n",
        "state = np.zeros((500,10)) + 25\n",
        "state_shape = state.shape\n",
        "\n",
        "# INITILIZING POLICY AND REWARD FUNCTION\n",
        "policy = PG(state_shape, n_actions)\n",
        "policy.to(DEVICE)\n",
        "norm_params = pd.read_csv('D:\\\\units\\\\thesis\\\\thesis\\\\train_reward_inverse_rl_w_sys_model\\\\norm_params.csv', header = None).to_numpy().T\n",
        "\n",
        "\n",
        "\n",
        "###### PRE-TRAIN WITH EXISTING DATA THE POLICY\n",
        "N_EPOCH = 50\n",
        "optimizer= torch.optim.Adam(policy.parameters(), 1e-2, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1) \n",
        "criterion1 = nn.CrossEntropyLoss()\n",
        "criterion2 = nn.MSELoss()\n",
        "\n",
        "\n",
        "# LOADING EXPERT/DEMO SAMPLES\n",
        "DATASETNPY_TRAIN = \"dataset_train.npy\"\n",
        "DATASETNPY_TEST = \"dataset_test.npy\"\n",
        "\n",
        "train_set = SeriesDataset(DATASETNPY_TRAIN)\n",
        "test_set = SeriesDataset(DATASETNPY_TEST)\n",
        "\n",
        "#print(\"Train set size: \" + str(len(train_set)))\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_set,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        test_set,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "\n",
        "for epoch in range(1, N_EPOCH + 1):\n",
        "    train(policy, epoch)\n",
        "    test(policy, epoch)\n",
        "    scheduler.step()\n",
        "\n",
        "##### TUNE MODEL WITH NEW LOSS\n",
        "\n",
        "mean_rewards = []\n",
        "mean_loss = []\n",
        "mean_loss_rew = []\n",
        "EPISODES_TO_PLAY = 10\n",
        "sample_trajs = []\n",
        "\n",
        "return_list, sum_of_loss_list = [], []\n",
        "\n",
        "for child in policy.children():\n",
        "    print(child)\n",
        "    k=1\n",
        "    for param in child.parameters():\n",
        "            if k % 4 == 0:\n",
        "                param.requires_grad = False\n",
        "            k+=1\n",
        "\n",
        "optimizer_policy = torch.optim.Adam(filter(lambda p: p.requires_grad, policy.parameters()), 1e-1, weight_decay=1e-2)\n",
        "for i in range(EPISODES_TO_PLAY):\n",
        "\n",
        "    states_init = np.zeros((1,500, 10)) + np.random.randint(20, 34, size=(10))\n",
        "    states_init = torch.FloatTensor(states_init)\n",
        "    output = policy(states_init) # forward pass\n",
        "    logits = output[:,:,0:4]\n",
        "    durations = output[:,:,4].detach().numpy()\n",
        "\n",
        "    durations = np.rint(durations) \n",
        "    probs = nn.functional.softmax(logits, -1) # get estimated actions for states\n",
        "    #log_probs = nn.functional.log_softmax(logits, -1)\n",
        "    \n",
        "    actions_probs_policy = np.squeeze(probs).detach().numpy()\n",
        "    # take actions only until first appearnce of duration 0\n",
        "    dur0_idx = np.where(durations == 0)\n",
        "    actions_probs_policy = actions_probs_policy[:dur0_idx[1][0]] \n",
        "\n",
        "    durations = np.transpose(durations)\n",
        "    durations = durations[:dur0_idx[1][0]]   \n",
        "    \n",
        "    actions_ind = []\n",
        "    actions = []\n",
        "    for prob in actions_probs_policy:                  \n",
        "        actions_ind.append(np.random.choice(4,  p = prob))\n",
        "\n",
        "    # actions was an array of individual type of actions to take, needs to be expanded with durations\n",
        "    actions_ind = np.array(actions_ind)\n",
        "    actions = np.repeat(actions_ind,durations[:,0].astype(int))\n",
        "    # Apply dict to go from 1,2,3,4 as action to [0,2400,0] [0,2400,1000]  [2300,0,0]  [2300,0,1000] \n",
        "    my_dict = {0:[0,2400,0], 1:[0,2400,1000], 2:[2300,0,0], 3:[2300,0,1000]}\n",
        "    actions_array = np.zeros((500,3))  \n",
        "    actions_array = map(my_dict.get, actions[1:500])\n",
        "\n",
        "    actions_array = np.array(list(actions_array))\n",
        "    actions_array_sysmodel = np.concatenate([np.arange(actions_array.shape[0])[:,None]+1,actions_array], axis=1) \n",
        "    actions_array_sysmodel = tf.expand_dims(actions_array_sysmodel, axis = 0, name=None)\n",
        "\n",
        "    states = sysmodel(actions_array_sysmodel)\n",
        "\n",
        "    states = states*norm_params[:,0]+norm_params[:,1]\n",
        "\n",
        "    # Recursively get expected discounted rewards (rewards given by the current cost function)\n",
        "    rewards = get_rewards(states, actions)\n",
        "    cumulative_returns_np = np.array(get_expected_rewards(rewards, 0.9))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns_np, dtype=torch.float32)\n",
        "\n",
        "    actions_tensor = torch.from_numpy(np.array(actions))\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        probs[:, 0:dur0_idx[1][0]] * to_one_hot(actions_tensor, 4), dim=1)\n",
        "\n",
        "    #entropy = -torch.mean(torch.sum(probs_samp*log_probs_samp), dim = -1 )\n",
        "    #loss = -torch.mean(log_probs_for_actions*cumulative_returns - entropy*1e-2) # loss for the policy (isnt it the cost function output?)\n",
        "    #average reward baseline\n",
        "    cumulative_returns = (cumulative_returns - torch.mean(cumulative_returns))\n",
        "    #print(cumulative_returns)\n",
        "    #whitening baseline\n",
        "    #cumulative_returns = (cumulative_returns - torch.mean(cumulative_returns))/ (torch.std(cumulative_returns))\n",
        "    loss_policy = -log_probs_for_actions*cumulative_returns/1e-3\n",
        "    # UPDATING THE POLICY NETWORK\n",
        "    optimizer_policy.zero_grad()\n",
        "    loss_policy.sum().backward()\n",
        "    torch.nn.utils.clip_grad_norm_(policy.parameters(), 0.01)\n",
        "    optimizer_policy.step()\n",
        "\n",
        "    #returns = sum(rewards)\n",
        "    sum_of_loss = loss_policy.sum().detach().numpy()\n",
        "    print(f\"\\nLoss: {sum_of_loss}\\n\")\n",
        "    #print(f\"\\nCumulative return: {cumulative_returns}\\n\")\n",
        "    #return_list.append(returns)\n",
        "    sum_of_loss_list.append(sum_of_loss)\n",
        "    states_plot = np.squeeze(states)\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.title(f\"Loss per {EPISODES_TO_PLAY} episodes\")\n",
        "    plt.plot(sum_of_loss_list)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.title(f\"Rewards\")\n",
        "    plt.plot(rewards)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.title(f\"Activations\")\n",
        "    plt.plot(actions[450:500])\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.title(f\"Temperatures\")\n",
        "    plt.plot(states_plot[0:50,:])\n",
        "    plt.grid()\n",
        "\n",
        "    # plt.show()\n",
        "    plt.savefig('GCL_learning_curve.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.6 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2276c0bad7efeb8779f7fdd76186059b8fbce5d8b401674f0514107ca347933d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
